\section { Computer Vision }
	\subsection { The pinhole camera model }
		Cameras project 3-dimensional scenes onto a 2-dimensional plane - the image plane.
		The overall projection of a single point in space (given in a world copordinate system) to the image plane can be divided into two independent successive operations \cite[S.~50]{Szeliski2010}:
		
		\subsubsection{Extrinsic parameters}
		A camera's extrinsic parameters describe its position and pose relative to the world coordinate system. For a simple pinhole camera those parameters are given with 
		
		\begin{equation}
		\matr{C} = \begin{bmatrix}
					\matr{R}_{3\times 3} & T_{3\times 1}\\
					0_{1\times 3} & 1
					\end{bmatrix}_{4\times 4} = -\matr{R}^{-1}T = -\matr{R}^TT \, . 
		\end{equation}
		$\matr{R}_{3\times 3}$ is a rotation matrix that represents the camera's rotation relative to the world coordinate system and T is the position of the world coordinate system, given in the new, camera-centered coordinate system. 

		\subsection{Intrinsic parameters}
		A camera intrinsic parameters reflect its internal structure and depend on focal length $f_{2\times 1}$, size of the image sensor and the principal point $c_{2\times1}$ which is defined as the intersection of the camera's optical axis with the image plane, denoted in pixel coordinates. This point ideally corresponds with the image sensor's center but might be slightly offset because of small misalignments within the camera.
		
		All in all this results in the camera's intrinsic matrix
		\begin{equation}
		\matr{K} =	\begin{bmatrix}
			f_x & s & c_x & 0 \\
			0 & f_y & c_y & 0 \\
			0 & 0 & 1 & 0
			\end{bmatrix}.
		\end{equation}
		The factor $s$ encodes any possible skew that might be introduced through misalignments between the camera's sensor and its optical axis and can typically be set to $0$ without affecting the approximation of the camera model too much.
		
		\subsection{Combined model}
		Combined, the intrinsic and extrinsic matrices lead to a camera matrix \cite{Szeliski2010}:
		
		\begin{equation}
		\matr{P} = \begin{bmatrix}\matr{K} & 0\\\matr{0}^T & 1\end{bmatrix}
		\begin{bmatrix}\matr{R} & T\\\matr{0}^T & 1\end{bmatrix}
		\end{equation}
		
		which maps a point $p_w = \left(x_w, y_w, z_w, 1\right)$ given in world coordinates to screen coordinates and disparity $d$, $p_s = \left(x_s, y_s, 1, d\right)$.
		

	
\section { Machine Learning }
	
	\subsection{Nearest Neighbors}
	(k-)Nearest Neighbor (\markup{k-NN}\abbrev{k-NN}{k-Nearest Neighbors}) algorithms as described in \cite{Cover1967, Fix1952} are based on the assumption that samples which are close in a (appropriate) metric will belong to the same class. Hence they provide a simple decision rule for classification problems where no underlying distribution of the sample points is known. 
	
	Given a set of training vectors in a multidimensional feature space, each with their corresponding class label, the k-NN algorithm uses majority-voting among $k$ of the closest examples from the training data to determine the class of a new, unknown example.
	
	A common distance metric for k-NN is the euclidean distance but depending on the structure of the data to be classified, other distance metrics (like the Hamming Distance for texts) may be used.
	
	With growing size of the example pool, naive implementations of k-NN become too computationally intensive since for every sample the distance to every availabe training sample has to be calculated. This effect can be reduced by pre-partitioning the training data in such a way that parts of the training data can be ignored when classifying a new sample. One example for this kind of pre-partitioning is the \textit{Ball Tree} or \textit{Metric Tree} which 
	
	\subsection { Neural Networks }
	Artificial neural networks 	are modeled on the biological function of human nerve tracts.	Early steps towards development and application of an artificial model of human neurons were made between 1943 and 1958 \cite{McCulloch1943, ROSENBLATT1958}. In 1958, Rosenblatt introduced the perceptron, a binary classifier mapping an input vector $\vec{x}$ to two classes, 1 and 0:
	
	\begin{equation}
	\label{eq:rosenblatt-perceptron}ø
	f(\vec{x}) =
	\begin{cases}
	1\text{, if } \vec{w}^{\,T}\vec{x} + b \geq 0 \\
	0\text{, else.}
	\end{cases}
	\end{equation}
	
	Depending on the weights $\vec{w}$ and the bias $b$, the perceptron can either "`fire"' (produce 1 as output) or "`not fire"' (produce 0 as output) for a given input vector $\vec{x}$. This concept is inspired by biological neurons which only fire an action potential if the amount of incoming stimuli reaches a certain threshold.
	
	Due to its linear nature, this model is not able to reliably predict the outcome of real-world processes, which are mostly non-linear by nature. In 1962, Rosenblatt was able to overcome this limitation by stacking perceptrons into layers and chaining two or more layers to create a \markup{MLP}\nomenclature{MLP}{Multilayer Perceptron} \cite{ROSENBLATT1961}. 

		\begin{figure}
					\input{Ressourcen/simple_perceptron}
					\centering
			\caption{Structure of a simple multi layer perceptron with one hidden layer.}
			\label{fig:mlp}
		\end{figure}

		In an \markup{MLP}, all neurons of one layer are fully connected to every neuron of the following layer via a weighted connection as shown in Figure \ref{fig:mlp}.
		
		Each arithmetic unit in layer $j$ takes the data $o_{j-1} = o_i$ from the preceding neurons and calculates a preliminary output value $net_j$ by multiplying each input with the corresponding weights $w_{ij}$:
		
		 \ref{eq:perceptron_simple}. 
		
		\begin{equation}
			\label{eq:perceptron_simple}
			\text{net}_j = \sum_{i=1}^{n} w_{ij} \cdot o_i + b \, . 
		\end{equation}
		
		The weights $\matr{w}$ and bias values $\vec{b}$ are optimized in the training process to approximate the training data as close as possible and will not change after the training.
		
		
		The inputs for the next layer are then calculated by filtering the preliminary results through a non-linear activation function,
		
		\begin{equation}
		\label{eq:perceptron_act}
		o_j = \varphi\left(\text{net}_j\right) \, . 
		\end{equation}
		

		
		 
		\subsubsection { Activation Functions }
		Since the basic arithmetic operations in a neural network are linear in nature, an additional nonlinearity must be introduced in order to predict nonlinear correlations. 
		For this purpose different transmission functions $\varphi$ are used, some of which are explained in the following.\\

		
		\textbf{Threshold function}
		The threshold function is the original activation function for the perceptron, inspired by the findings of \cite{McCulloch1943} and has only $0$ and $1$ as possible initial values. With the threshold $\epsilon$ it is defined as
		\begin{equation}
		\label{eq:acti-sw}
		o_j = \left\{
		\begin{array}{ll}
		1\text{, wenn } \text{net}_j > \epsilon \\
		0 \text{ sonst}\\
		\end{array}
		\right.
		\end{equation}
		\textbf{Sigmoid}
			Die Sigmoid-Funktion ist mit einem variablen Steigungsparameter $a$ definiert zu 
			\begin{equation}
			\varphi\left(\text{net}_j\right) = \frac{1}{1+\exp(-a \cdot \text{net}_j)}
			\end{equation}
			Sie wird häufig anstatt der Schwellenwertfunktion genutzt, da sie stetig differenzierbar und somit gut geeignet für häufig genutzte Trainingsverfahren wie Gradient Descend ist.\\
			\begin{figure}[ht]
				\centering
				\begin{tikzpicture}
				\begin{axis}[
				domain=-200:200,
				xmin=-10, xmax=10,
				ymin=-1.5, ymax=1.5,
				samples=401,
				axis y line=center,
				axis x line=middle,
				]
				\addplot+[mark=none] {1/(1 + exp(-x)};
				\end{axis}
				\end{tikzpicture}
				\caption{Die Sigmoid-Funktion begrenzt die Ausgangswerte wie auch die Schwellenwertfunktion auf den Bereich [0, 1].}
				\label{fig:sigmoid_plot}
			\end{figure}


		\textbf{ReLu}
		Die Rectifying linear unit (kurz: ReLu) ist eine weitere Form der Aktivierungsfunktion, die insbesondere in Deep Neural Networks und Convolutional Neural Networks eingesetzt wird. Sie ist definiert zu
		\begin{equation}
			\label{eq:relu_def}
			\varphi(\text{net}_j) = \max(\text{net}_j, 0)
		\end{equation}
		womit negative Werte abgeschnitten werden. Im Vergleich zur Schwellenwert- bzw. Sigmoidfunktion führen große Eingangswerte hier nicht zur Sättigung (und damit kleinem Gradienten), was insbesondere in Gradientenverfahren wie in Abschnitt \ref{sec:gradient-descend} von Vorteil ist. \\
		
		\begin{figure}[ht]
			\centering
		\begin{tikzpicture}
		\begin{axis}[
		domain=-200:200,
		xmin=-10, xmax=10,
		ymin=-10, ymax=10,
		samples=401,
		axis y line=center,
		axis x line=middle,
		]
		\addplot+[mark=none] {max(x, 0)};
		\end{axis}
		\end{tikzpicture}
		\caption{Durch die ReLu-Aktivierungsfunktion werden negative Werte abgeschnitten.}
		\label{fig:relu_plot}
		\end{figure}

	
	\subsection { Spezialfälle Neuronaler Netze}
		\subsubsection { Convolutional Neural Networks (CNN) }
		Convolutional neural networks (\markup{CNN}\abbrev{CNN}{Convolutional Neural Network}) are a special case of neural networks that are particularly suitable for processing higher-dimensional structures such as images or temporal sequences of data. 

		In a CNN, in addition to the "`dense"' or "`fully connected"' layers described above, additional layers are used that perform convolution operations on the data.

		Instead of a simple multiplication, each input tensor is convoluted with a matrix in each convolution layer.

		In the case of a 2D CNN, the input data is a 2D matrix and each convolution layer also contains several two-dimensional matrices, which are moved over the input matrix to calculate the output data. 

		
		\todo{Bild für Faltung}
		
		
		\subsubsection { Recurrent Neural Networks (RNN) }
		\subsubsection { Long Short Term Memory (LSTM) }
	
	\subsection { Trainingsmethoden }
		
		\subsubsection{Gradient Descend}
		\label{sec:gradient-descend}
		
		\subsubsection{ADAM}
		\todo{Link zum Paper ist in tensorflow source von adam optimizer zu finden}
		
	\subsection{ One Shot Learning - Siamesische Netze }
	Ein Problem der bisher gezeigten Machine Learning-Methoden ist, dass sehr viele Traningsdaten benötigt werden, um die Netze ausreichend zu trainieren. In Anwendungsfällen, in denen die Trainingsdaten erst in der Benutzerinteraktion zur Verfügung stehen, können jedoch häufig nicht ausreichend viele Daten gesammelt werden. 
	
	Ein klassischer Anwendungsfall für One Shot learning ist daher zum Beispiel die Gesichtserkennung in Anwendersoftware. Hier muss es möglich sein, mit wenigen Gesichtern als Vorlage ein Gesicht auf einem neuen Photo zuverlässig wiederzuerkennen. Gleichzeitig sollte es jeder Zeit möglich sein, neue Klassen hinzuzufügen oder alte Klassen zu entfernen. Auch hier ist ein klassisches Netzwerk mit einer festen Anzahl von Ausgangsneuronen - und damit Klassen - ungeeignet.

	\subsubsection{Funktionsprinzip}
		Siamesische Netze sind eine mögliche Lösung für beide genannten Probleme: Sie vergleichen zwei Eingangstensoren und errechnen aus diesen einen Ähnlichkeitsfaktor. Anhand dieses Faktors kann nach Vergleich des unbekannten Eingangs mit allen bekannten Klassen diejenige mit der höchsten Übereinstimmung (oder keine im Fall eines Negativbeispiels) gewählt werden.
		
	\subsubsection{Aufbau des Netzes}
	 Bei einem Siamesischen Netzwerk handelt es sich um ein zweistufiges Netz, bestehend aus zwei Faltungsnetzen mit identischen Gewichten in der ersten Stufe, deren Ergebnis-Vektoren in der zweiten Stufe durch ein zusammenfassendes mehrlagiges Perzeptron in einen einzelnen Ähnlichkeits-Wert umgerechnet werden.
	 Die Faltungsnetze dienen dabei der Dimensionsreduktion - sie reduzieren den Eingangstensor auf einen Festure-Vektor und komprimieren damit die zur Verfügung stehende Information \todo{weiter}
	 
	 
	 
	 \begin{figure}
	 	\centering
	 	\input{Ressourcen/siamese_network}
	 	\caption{Aufbau eines siamesischen Netzwerkes}
	 	\label{fig:siamesenetwork}
	 \end{figure}
	 
	 
	 \subsubsection{Training}
	 Aufgrund des zweistufigen Aufbaus kann auch das Training des Netzes in zwei Stufen erfolgen.
	 
	 \subsubsection{Kostenfunktion: Triplet Loss}
	 Da das Netzwerk \todo{weiter}
	 

	
	Hierfür kann ein Siamesisches Netzwerk (en.: siamese network) genutzt werden. 
	
\section { Modeling the human hand }
	\abbrev{PIP}{Proximal Interphalangeal, see Figure \ref{fig:malik2018handmodel}}
	\abbrev{DIP}{Distal Interphalangeal, see Figure \ref{fig:malik2018handmodel}}
	\abbrev{MCP}{Metacarpophalangeal, see Figure \ref{fig:malik2018handmodel}}
	\abbrev{TIP}{Finger Tip}
	
	\begin{figure}
		\centering
		\includegraphics[width=0.7\linewidth]{Ressourcen/malik2018_hand_model}
		\caption[Handmodell nach \cite{Malik2018b}]{In \cite{Malik2018b} wird ein Modell ähnlich dem oben stehenden (Quelle: \cite{Malik2018b}) genutzt, in dem die vollständige Handpose durch 21, bzw. 22 (Gelenk-) Koordinaten bestimmt ist.}
		\label{fig:malik2018handmodel}
	\end{figure}
	Throughout this thesis, the hand model shown in Figure \ref{fig:malik2018handmodel} will be used as it provides enough key points to reconstruct hand poses to a sufficient degree.
	
	The freedom of movement of the individual joints of the hand is subject to anatomical restrictions. Modeling those restrictions can be useful to assess the quality of the estimate and to refine it accordingly. In \cite{Lin2000} the constraints are divided into two types: 
	
	Type 1 constraints describe the valid flexion and abduction/adduction angles
	
	\begin{alignat}{7}
		&&0\degree \quad &&\leq \quad &&\theta_{MCP_F} \quad &&\leq \quad &90\degree\, &&,\\
		&&0\degree \quad &&\leq \quad &&\theta_{PIP_F} \quad &&\leq \quad &110\degree\,&& ,\\
		&&0\degree \quad &&\leq \quad &&\theta_{DIP_F} \quad &&\leq \quad &90\degree\, && \text{and}\\
		&&-15\degree \quad &&\leq \quad &&\theta_{MCP_AA} \quad &&\leq \quad &15\degree\, &&,
	\end{alignat}
	
	\begin{equation}
	\theta_{MCP_AA} = 0\degree\, \text{and}
	\end{equation}

	\begin{equation}
	\theta_{TM_AA} = 0\degree\,.
	\end{equation}
	
	Type 2 constraints describe inter- and intra finger limits in motion:
	A natural motion of the proximal interphalangeal (\markup{PIP}) will cause a proportional motion in the corresponding distal interphalangeal (\markup{DIP}).
	\subsection{title}
	
	
\section{Kameraparameter}
\subsection{Intrinsische Parameter}
Die intrinsischen Paramter einer Kamera sind definiert durch die Brennweite $f$, das Format des Bildsensors und 
\subsection{Extrinsische Parameter}

\section{YOLO}
\markup{YOLO}\abbrev{YOLO}{You Only Look Once - Object detection Network} is a convolutional object detection network, described in \cite{Redmon2016}. Which uses an architecture similar to GoogLeNet \cite{Szegedy2014}. Instead of inception modules, as used in GoogLeNet, \cite{Redmon2016} uses building blocks with $1\times1$ reduction layers, followed by $3\times3$ convolutional layers. 

\begin{figure}[ht]
	\centering
	\includegraphics[width=0.7\linewidth]{Ressourcen/yolo-net}
	\caption[Architecture of the YOLO network.]{Architecture of the YOLO network. Source: \cite{Redmon2016}}
	\label{fig:yolo-net}
\end{figure}


	