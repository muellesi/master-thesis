% This file was created with Citavi 6.3.15.0

@proceedings{2001,
 year = {13-17 March 2001},
 title = {{Proceedings IEEE Virtual Reality 2001}},
 publisher = {{IEEE Comput. Soc}},
 isbn = {0-7695-0948-7}
}


@proceedings{2001b,
 year = {13-17 March 2001},
 title = {{Proceedings IEEE Virtual Reality 2001}},
 publisher = {{IEEE Comput. Soc}},
 isbn = {0-7695-0948-7}
}


@book{2006,
 year = {2006},
 title = {{Proceedings of the 5th international conference on devlopment and learning ICDL 2006: Indiana University, Bloomington, IN : May 31 - June 3rd, 2006}},
 address = {[Bloomington, Ind.]},
 publisher = {{Department of Psychological and Brain Sciences, Indiana University}},
 isbn = {9780978645601}
}


@proceedings{2011,
 year = {2011},
 isbn = {1938-7228}
}


@proceedings{2013,
 year = {2013},
 title = {{Proceedings of the IEEE International Conference on Computer Vision (ICCV)}}
}


@proceedings{2014,
 year = {2014},
 title = {{2013 IEEE International Conference on Computer Vision: 1-8 December 2013}},
 keywords = {Computer vision},
 address = {New York},
 publisher = {IEEE},
 isbn = {978-1-4799-2840-8}
}


@proceedings{2016,
 year = {27.06.2016 - 30.06.2016},
 title = {{2016 IEEE Conference on Computer Vision and Pattern Recognition (CVPR)}},
 publisher = {IEEE},
 isbn = {978-1-4673-8851-1}
}


@proceedings{2019,
 year = {2019}
}


@misc{AsadiAghbolaghi20180108,
 abstract = {Interest in automatic action and gesture recognition has grown considerably in the last few years. This is due in part to the large number of application domains for this type of technology. As in many other computer vision areas, deep learning based methods have quickly become a reference methodology for obtaining state-of-the-art performance in both tasks. This chapter is a survey of current deep learning based methodologies for action and gesture recognition in sequences of images. The survey reviews both fundamental and cutting edge methodologies reported in the last few years. We introduce a taxonomy that summarizes important aspects of deep learning for approaching both tasks. Details of the proposed architectures, fusion strategies, main datasets, and competitions are reviewed. Also, we summarize and discuss the main works proposed so far with particular interest on how they treat the temporal dimension of data, their highlighting features, and opportunities

and challenges for future research. To the best of our knowledge this is the first survey in the topic. We foresee this survey will become a reference in this ever dynamic field of research.},
 author = {Asadi-Aghbolaghi, Maryam and Clapes, Albert and Bellantonio, Marco and Escalante, Hugo and Ponce-L{\'o}pez, V{\'i}ctor and Bar{\'o}, Xavier and Guyon, Isabelle and Kasaei, Shohreh and Escalera, Sergio},
 year = {2017},
 title = {{Deep learning for action and gesture recognition in image sequences: a survey}},
 url = {https://hal.inria.fr/hal-01678006/document},
 keywords = {Action recognition;Deep Learning Architectures;Fusion Strategies;Gesture Recognition}
}


@inproceedings{Berlemont2015,
 crossref = {InstituteofElectricalandElectronicsEngineers2015b},
 author = {Berlemont, Samuel and Lefebvre, Gregoire and Duffner, Stefan and Garcia, Christophe},
 title = {{Siamese neural network based similarity metric for inertial gesture classification and rejection}},
 pages = {1--6},
 publisher = {IEEE},
 isbn = {978-1-4799-6026-2},
 booktitle = {{2015 11th IEEE International Conference and Workshops on Automatic Face and Gesture Recognition (FG)}},
 year = {2015},
 address = {Piscataway, NJ},
 doi = {10.1109/FG.2015.7163112}
}


@incollection{Berlemont2016,
 crossref = {Villa2016},
 author = {Berlemont, Samuel and Lefebvre, Gr{\'e}goire and Duffner, Stefan and Garcia, Christophe},
 title = {{Polar Sine Based Siamese Neural Network for Gesture Recognition}},
 pages = {406--414},
 volume = {9887},
 publisher = {Springer},
 isbn = {978-3-319-44780-3},
 series = {{Lecture Notes in Computer Science}},
 editor = {Villa, Alessandro E.P. and Maulli, Paolo and {Pons Rivero}, Antonio Javier},
 booktitle = {{Artificial neural networks and machine learning - ICANN 2016}},
 year = {2016},
 address = {Switzerland},
 doi = {10.1007/978-3-319-44781-0{\textunderscore }48}
}


@book{Bishop2009,
 author = {Bishop, Christopher M.},
 year = {2009},
 title = {{Pattern recognition and machine learning}},
 url = {http://www.loc.gov/catdir/enhancements/fy0818/2006922522-d.html},
 price = {hardback : EUR 75,92},
 address = {New York, NY},
 edition = {Corrected at 8th printing 2009},
 publisher = {Springer},
 isbn = {9780387310732},
 series = {{Information science and statistics}}
}


@book{Bourdot2018,
 year = {2018},
 title = {{Virtual reality and augmented reality: 15th EuroVR International Conference, EuroVR 2018, London, UK, October 22--23, 2018 : proceedings}},
 price = {(Festeinband : EUR 62.05 (DE) (freier Preis), EUR 63.79 (AT) (freier Preis), CHF 64.00 (freier Preis))},
 keywords = {Erweiterte Realit{\"a}t;Erweiterte Realit{\"a}t {\textless}Informatik{\textgreater};Virtuelle Realit{\"a}t;Virtuelle Welten},
 address = {Cham},
 volume = {11162},
 publisher = {Springer},
 isbn = {978-3-030-01789-7},
 series = {{Lecture Notes in Computer Science}},
 editor = {Bourdot, Patrick and Cobb, Sue and Interrante, Victoria and kato, Hirokazu and Stricker, Didier},
 doi = {10.1007/978-3-030-01790-3}
}


@article{Cover1967,
 author = {Cover, T. and Hart, P.},
 year = {1967},
 title = {{Nearest neighbor pattern classification}},
 pages = {21--27},
 volume = {13},
 number = {1},
 issn = {0018-9448},
 journal = {{IEEE Transactions on Information Theory}},
 doi = {10.1109/TIT.1967.1053964}
}


@book{Cremers2015,
 year = {2015},
 title = {{Computer Vision -- ACCV 2014}},
 address = {Cham},
 publisher = {{Springer International Publishing}},
 isbn = {978-3-319-16864-7},
 series = {{Lecture Notes in Computer Science}},
 editor = {Cremers, Daniel and Reid, Ian and Saito, Hideo and Yang, Ming-Hsuan},
 doi = {10.1007/978-3-319-16865-4}
}


@book{Cremers2015b,
 year = {2015},
 title = {{Computer Vision -- ACCV 2014}},
 address = {Cham},
 publisher = {{Springer International Publishing}},
 isbn = {978-3-319-16864-7},
 series = {{Lecture Notes in Computer Science}},
 editor = {Cremers, Daniel and Reid, Ian and Saito, Hideo and Yang, Ming-Hsuan},
 doi = {10.1007/978-3-319-16865-4}
}


@misc{Denker1988,
 author = {Denker, J. S. and Gardner, W. R. and Graf, H. P. and Henderson, D. and Howard, R. E. and Hubbard, W. and Jackel, L. D. and Baird, H. S. and Guyon, I.},
 year = {1988},
 title = {{Neural network recognizer for hand-written zip code digits}},
 publisher = {{MIT Press}}
}


@inproceedings{Devineau2018,
 crossref = {IEEEInternationalConferenceonAutomaticFaceandGestureRecognition2018},
 author = {Devineau, Guillaume and Moutarde, Fabien and Xi, Wang and Yang, Jie},
 title = {{Deep Learning for Hand Gesture Recognition on Skeletal Data}},
 pages = {106--113},
 publisher = {IEEE},
 isbn = {978-1-5386-2335-0},
 booktitle = {{13th IEEE International Conference on Automatic Face and Gesture Recognition}},
 year = {2018},
 address = {Piscataway, NJ},
 doi = {10.1109/FG.2018.00025}
}


@misc{Dietz2019,
 author = {Dietz, Florian},
 editor = {towardsdatascience.com},
 year = {09.11.2019},
 title = {{Why your AI might be racist and what to do about it}},
 url = {https://towardsdatascience.com/why-your-ai-might-be-racist-and-what-to-do-about-it-c081288f600a},
 urldate = {11.12.2019}
}


@article{DiWu2016,
 abstract = {This paper describes a novel method called Deep Dynamic Neural Networks (DDNN) for multimodal gesture recognition. A semi-supervised hierarchical dynamic framework based on a Hidden Markov Model (HMM) is proposed for simultaneous gesture segmentation and recognition where skeleton joint information, depth and RGB images, are the multimodal input observations. Unlike most traditional approaches that rely on the construction of complex handcrafted features, our approach learns high-level spatio-temporal representations using deep neural networks suited to the input modality: a Gaussian-Bernouilli Deep Belief Network (DBN) to handle skeletal dynamics, and a 3D Convolutional Neural Network (3DCNN) to manage and fuse batches of depth and RGB images. This is achieved through the modeling and learning of the emission probabilities of the HMM required to infer the gesture sequence. This purely data driven approach achieves a Jaccard index score of 0.81 in the ChaLearn LAP gesture spotting challenge. The performance is on par with a variety of state-of-the-art hand-tuned feature-based approaches and other learning-based methods, therefore opening the door to the use of deep learning techniques in order to further explore multimodal time series data.},
 author = {{Di Wu} and Pigou, Lionel and Kindermans, Pieter-Jan and Le, Nam Do-Hoang and Shao, Ling and Dambre, Joni and Odobez, Jean-Marc},
 year = {2016},
 title = {{Deep Dynamic Neural Networks for Multimodal Gesture Segmentation and Recognition}},
 keywords = {Algorithms;Gestures;Humans;Learning;Neural Networks (Computer);Normal Distribution;Pattern Recognition, Automated},
 pages = {1583--1597},
 volume = {38},
 number = {8},
 journal = {{IEEE transactions on pattern analysis and machine intelligence}},
 doi = {10.1109/TPAMI.2016.2537340}
}


@article{Farabet2013,
 abstract = {Scene labeling consists of labeling each pixel in an image with the category of the object it belongs to. We propose a method that uses a multiscale convolutional network trained from raw pixels to extract dense feature vectors that encode regions of multiple sizes centered on each pixel. The method alleviates the need for engineered features, and produces a powerful representation that captures texture, shape, and contextual information. We report results using multiple postprocessing methods to produce the final labeling. Among those, we propose a technique to automatically retrieve, from a pool of segmentation components, an optimal set of components that best explain the scene; these components are arbitrary, for example, they can be taken from a segmentation tree or from any family of oversegmentations. The system yields record accuracies on the SIFT Flow dataset (33 classes) and the Barcelona dataset (170 classes) and near-record accuracy on Stanford background dataset (eight classes), while being an order of magnitude faster than competing approaches, producing a {\$}(320$\backslash$$\backslash$times 240){\$} image labeling in less than a second, including feature extraction.},
 author = {Farabet, Cl{\'e}ment and Couprie, Camille and Najman, Laurent and Lecun, Yann},
 year = {2013},
 title = {{Learning hierarchical features for scene labeling}},
 pages = {1915--1929},
 volume = {35},
 number = {8},
 journal = {{IEEE transactions on pattern analysis and machine intelligence}},
 doi = {10.1109/TPAMI.2012.231}
}


@article{Farooq2015,
 author = {Farooq, Adnan and Won, Chee Sun},
 year = {2015},
 title = {{A Survey of Human Action Recognition Approaches that use an RGB-D Sensor}},
 pages = {281--290},
 volume = {4},
 number = {4},
 journal = {{IEIE Transactions on Smart Processing and Computing}},
 doi = {10.5573/IEIESPC.2015.4.4.281}
}


@article{FeiFei,
 author = {Fei-Fei, Li},
 title = {{Knowledge transfer in learning to recognize visual objects classes}}
}


@book{Fitzgibbon2012,
 year = {2012},
 title = {{Computer vision - ECCV 2012: 12th European Conference on Computer Vision, Florence, Italy, October 7 - 13, 2012 ; proceedings, part VI}},
 keywords = {Artificial intelligence;Computer graphics;Computer science;Computer software;Computer vision;Optical pattern recognition},
 address = {Berlin},
 volume = {7577},
 publisher = {Springer},
 isbn = {978-3-642-33782-6},
 series = {{Lecture Notes in Computer Science}},
 editor = {Fitzgibbon, Andrew and Lazebnik, Svetlana and Perona, Pietro and Sato, Yoichi and Schmid, Cordelia},
 doi = {10.1007/978-3-642-33783-3}
}


@misc{Fix1952,
 author = {Fix, Evelyn and Hodges, J. L.},
 year = {1952},
 title = {{Discriminatory Analysis: Nonparametric Discrimination: Small Sample Performance}},
 url = {https://books.google.de/books?hl=de&lr=&id=vlHQAAAAMAAJ&oi=fnd&pg=PP4&dq=E.+Fix+and+J.L.+Hodges,+Discriminatory+analysis,+non-parametric+discrimination.+USAF+School+of+Aviation+Medicine,+Randolph+Field,+TX,+Project+21-49-004,+Report+4,+Contract+AF41(128)-3,+1951.+&ots=nWMQsUZnrx&sig=MDswOPNfaRCd4eu09t_PAGUj-jU#v=onepage&q&f=false},
 address = {Randolph Field, TX},
 urldate = {03.12.2019},
 number = {11},
 editor = {{University of California}, Berkeley},
 institution = {{USAF School of Aviation Medicine}}
}


@article{Fix1989,
 author = {Fix, Evelyn and Hodges, J. L.},
 year = {1989},
 title = {{Discriminatory Analysis. Nonparametric Discrimination: Consistency Properties}},
 pages = {238},
 volume = {57},
 number = {3},
 issn = {03067734},
 journal = {{International Statistical Review / Revue Internationale de Statistique}},
 doi = {10.2307/1403797}
}


@misc{GarciaHernando2017,
 abstract = {In this work we study the use of 3D hand poses to recognize first-person dynamic hand actions interacting with 3D objects. Towards this goal, we collected RGB-D video sequences comprised of more than 100K frames of 45 daily hand action categories, involving 26 different objects in several hand configurations. To obtain hand pose annotations, we used our own mo-cap system that automatically infers the 3D location of each of the 21 joints of a hand model via 6 magnetic sensors and inverse kinematics. Additionally, we recorded the 6D object poses and provide 3D object models for a subset of hand-object interaction sequences. To the best of our knowledge, this is the first benchmark that enables the study of first-person hand actions with the use of 3D hand poses. We present an extensive experimental evaluation of RGB-D and pose-based action recognition by 18 baselines/state-of-the-art approaches. The impact of using appearance features, poses, and their combinations are measured, and the different training/testing protocols are evaluated. Finally, we assess how ready the 3D hand pose estimation field is when hands are severely occluded by objects in egocentric views and its influence on action recognition. From the results, we see clear benefits of using hand pose as a cue for action recognition compared to other data modalities. Our dataset and experiments can be of interest to communities of 3D hand pose estimation, 6D object pose, and robotics as well as action recognition.},
 author = {Garcia-Hernando, Guillermo and Yuan, Shanxin and Baek, Seungryul and Kim, Tae-Kyun},
 date = {4/8/2017},
 title = {{First-Person Hand Action Benchmark with RGB-D Videos and 3D Hand Pose  Annotations}},
 url = {http://arxiv.org/pdf/1704.02463v2},
 keywords = {Computer Science - Computer Vision and Pattern Recognition}
}


@misc{Ge2016,
 abstract = {Articulated hand pose estimation plays an important role in human-computer interaction. Despite the recent progress, the accuracy of existing methods is still not satisfactory, partially due to the difficulty of embedded high-dimensional and non-linear regression problem. Different from the existing discriminative methods that regress for the hand pose with a single depth image, we propose to first project the query depth image onto three orthogonal planes and utilize these multi-view projections to regress for 2D heat-maps which estimate the joint positions on each plane. These multi-view heat-maps are then fused to produce final 3D hand pose estimation with learned pose priors. Experiments show that the proposed method largely outperforms state-of-the-art on a challenging dataset. Moreover, a cross-dataset experiment also demonstrates the good generalization ability of the proposed method.},
 author = {Ge, Liuhao and Liang, Hui and Yuan, Junsong and Thalmann, Daniel},
 date = {6/23/2016},
 title = {{Robust 3D Hand Pose Estimation in Single Depth Images: from Single-View  CNN to Multi-View CNNs}},
 url = {http://arxiv.org/pdf/1606.07253v3},
 keywords = {Computer Science - Computer Vision and Pattern Recognition}
}


@inproceedings{Ge2017,
 crossref = {IEEEConferenceonComputerVisionandPatternRecognition2017},
 author = {Ge, Liuhao and Liang, Hui and Yuan, Junsong and Thalmann, Daniel},
 title = {{3D Convolutional Neural Networks for Efficient and Robust Hand Pose Estimation from Single Depth Images}},
 pages = {5679--5688},
 publisher = {IEEE},
 isbn = {978-1-5386-0457-1},
 booktitle = {{30th IEEE Conference on Computer Vision and Pattern Recognition}},
 year = {2017},
 address = {Piscataway, NJ},
 doi = {10.1109/CVPR.2017.602}
}


@article{Ge2019,
 abstract = {In this paper, we present a novel method for real-time 3D hand pose estimation from single depth images using 3D Convolutional Neural Networks (CNNs). Image-based features extracted by 2D CNNs are not directly suitable for 3D hand pose estimation due to the lack of 3D spatial information. Our proposed 3D CNN-based method, taking a 3D volumetric representation of the hand depth image as input and extracting 3D features from the volumetric input, can capture the 3D spatial structure of the hand and accurately regress full 3D hand pose in a single pass. In order to make the 3D CNN robust to variations in hand sizes and global orientations, we perform 3D data augmentation on the training data. To further improve the estimation accuracy, we propose applying the 3D deep network architectures and leveraging the complete hand surface as intermediate supervision for learning 3D hand pose from depth images. Extensive experiments on three challenging datasets demonstrate that our proposed approach outperforms baselines and state-of-the-art methods. A cross-dataset experiment also shows that our method has good generalization ability. Furthermore, our method is fast as our implementation runs at over 91 frames per second on a standard computer with a single GPU.},
 author = {Ge, Liuhao and Liang, Hui and Yuan, Junsong and Thalmann, Daniel},
 year = {2019},
 title = {{Real-Time 3D Hand Pose Estimation with 3D Convolutional Neural Networks}},
 pages = {956--970},
 volume = {41},
 number = {4},
 journal = {{IEEE transactions on pattern analysis and machine intelligence}},
 doi = {10.1109/TPAMI.2018.2827052}
}


@book{Gero1996,
 abstract = {The chapters in this volume are from the Fourth International Conference on Artificial Intelligence in Design held in June 1996 in Stanford, California. They represent the state-of-the-art and the cutting edge of research and development in this field. They are of particular interest to researchers, developers and users of computer systems in design. This volume demonstrates both the breadth and depth of artificial intelligence in design and points the way forward for our understanding of design as a process and for the development of computer-based tools to aid designers. The chapters describe advances in both theory and application in the following areas: Case-based design; Conceptual design; creativity and innovation in design; Design objects; Design spaces; Distributed design; Genetic algorithms/Genetic programming in design; Grammars in design; Learning in design; Representations in design; Reuse of designs; Rules; Models and theories in design; Layout planning in design},
 year = {1996},
 title = {{Artificial Intelligence in Design '96}},
 address = {Dordrecht},
 publisher = {{Springer Netherlands}},
 isbn = {978-94-009-0279-4},
 editor = {Gero, John S. and Sudweeks, Fay},
 doi = {10.1007/978-94-009-0279-4}
}


@inproceedings{Glorot2011,
 crossref = {2011},
 author = {Glorot, Xavier and Bordes, Antoine and Bengio, Yoshua},
 title = {{Deep Sparse Rectifier Neural Networks}},
 url = {http://proceedings.mlr.press/v15/glorot11a/glorot11a.pdf},
 pages = {315--323},
 isbn = {1938-7228},
 year = {2011}
}


@article{Hafiz2015,
 abstract = {Complex-valued neural networks (CVNNs), that allow processing complex-valued data directly, have been applied to a number of practical applications, especially in signal and image processing. In this paper, we apply CVNN as a classification algorithm for the skeletal wireframe data that are generated from hand gestures. A CVNN having one hidden layer that maps complex-valued input to real-valued output was used, a training algorithm based on Levenberg Marquardt algorithm (CLMA) was derived, and a task to recognize 26 different gestures that represent English alphabet was given. The initial image processing part consists of three modules: real-time hand tracking, hand-skeletal construction, and hand gesture recognition. We have achieved; (1) efficient and accurate gesture extraction and representation in complex domain, (2) training of the CVNN utilising CLMA, and (3) providing a proof of the superiority of the aforementioned methods by utilising complex-valued learning vector quantization. A comparison with real-valued neural network shows that a CVNN with CLMA provides higher recognition performance, accompanied by significantly faster training. Moreover, a comparison of six different activation functions was performed and their utility is argued.},
 author = {Hafiz, Abdul Rahman and Al-Nuaimi, Ahmed Yarub and Amin, Md. Faijul and Murase, Kazuyuki},
 year = {2015},
 title = {{Classification of Skeletal Wireframe Representation of Hand Gesture Using Complex-Valued Neural Network}},
 url = {https://doi.org/10.1007/s11063-014-9379-0},
 pages = {649--664},
 volume = {42},
 number = {3},
 issn = {1573-773X},
 journal = {{Neural Processing Letters}},
 doi = {10.1007/s11063-014-9379-0}
}


@misc{Hampali2019,
 abstract = {We propose a new dataset for 3D hand+object pose estimation from color images, together with a method for efficiently annotating this dataset, and a 3D pose prediction method based on this dataset. The current lack of training data makes the 3D hand+object pose estimation very challenging. This lack is due to the complexity of labeling many real images with both 3D poses and of generating synthetic images with various realistic interaction. Moreover, even if synthetic images could be used for training, annotated real images are still needed for validation. To tackle this challenge, we capture sequences with a simple setup made of a single RGB-D camera. We also use a color camera imaging the sequences from a side view, but only for validation. We introduce a novel method based on global optimization that exploits depth, color, and temporal constraints for efficiently annotating the sequences, which we use to train another novel method that predicts both the 3D poses of the hand and the object from a single color image. Our hope is to encourage other researchers to develop better annotation methods for our dataset: One can then apply such method to capture and easily annotate sequences captured with a single RGB-D camera to easily create additional training data thus solving one of the main problems of 3D hand+object pose estimation.},
 author = {Hampali, Shreyas and Oberweger, Markus and Rad, Mahdi and Lepetit, Vincent},
 date = {7/2/2019},
 title = {{HO-3D: A Multi-User, Multi-Object Dataset for Joint 3D Hand-Object Pose  Estimation}},
 url = {http://arxiv.org/pdf/1907.01481v1},
 keywords = {Computer Science - Computer Vision and Pattern Recognition}
}


@inproceedings{HernandezRuiz2017,
 crossref = {Liu2017},
 author = {{Hernandez Ruiz}, Alejandro and Porzi, Lorenzo and {Rota Bul{\`o}}, Samuel and Moreno-Noguer, Francesc},
 title = {{3D CNNs on Distance Matrices for Human Action Recognition}},
 pages = {1087--1095},
 publisher = {{ACM Association for Computing Machinery}},
 isbn = {9781450349062},
 editor = {Liu, Qiong and Lienhart, Rainer and Wang, Haohong and Chen, Sheng-Wei {\textquotedbl}Kuan-Ta{\textquotedbl} and Boll, Susanne and Chen, Phoebe and Friedland, Gerald and Li, Jia and Yan, Shuicheng},
 booktitle = {{MM'17}},
 year = {2017},
 address = {New York, NY, USA},
 doi = {10.1145/3123266.3123299}
}


@inproceedings{Hoang2018,
 crossref = {Unknown2018},
 author = {Hoang, Nguyen Ngoc and Lee, Guee-Sang and Kim, Soo-Hyung and Yang, Hyung-Jeong},
 title = {{A Real-time Multimodal Hand Gesture Recognition via 3D Convolutional Neural Network and Key Frame Extraction}},
 pages = {32--37},
 publisher = {{ACM Press}},
 isbn = {9781450365567},
 editor = {Unknown},
 booktitle = {{Proceedings of the 2018 International Conference on Machine Learning and Machine Intelligence  - MLMI2018}},
 year = {2018},
 address = {New York, New York, USA},
 doi = {10.1145/3278312.3278314}
}


@proceedings{Hoey2011,
 year = {2011},
 title = {{Proceedings of the British Machine Vision Conference 2011: BMVC 2011, the 22nd British Machine Vision Conference, University of Dundee, 29 August-2 September 2011}},
 address = {Durham},
 publisher = {{BMVA Press}},
 isbn = {1-901725-43-X},
 editor = {Hoey, Jesse},
 institution = {{British Machine Vision Conference} and BMVC},
 doi = {10.5244/C.25}
}


@proceedings{IEEEConferenceonComputerVisionandPatternRecognition2016,
 year = {2016},
 title = {{29th IEEE Conference on Computer Vision and Pattern Recognition: CVPR 2016 : proceedings : 26 June-1 July 2016, Las Vegas, Nevada}},
 keywords = {Maschinelles Sehen;Mustererkennung},
 address = {Piscataway, NJ},
 publisher = {IEEE},
 isbn = {978-1-4673-8851-1},
 institution = {{IEEE Conference on Computer Vision and Pattern Recognition} and {Institute of Electrical and Electronics Engineers} and CVPR}
}


@proceedings{IEEEConferenceonComputerVisionandPatternRecognition2017,
 year = {2017},
 title = {{30th IEEE Conference on Computer Vision and Pattern Recognition: CVPR 2017 : 21-26 July 2016, Honolulu, Hawaii : proceedings}},
 keywords = {Maschinelles Sehen;Mustererkennung},
 address = {Piscataway, NJ},
 publisher = {IEEE},
 isbn = {978-1-5386-0457-1},
 institution = {{IEEE Conference on Computer Vision and Pattern Recognition} and {Institute of Electrical and Electronics Engineers} and {IEEE/CVF Conference on Computer Vision and Pattern Recognition} and CVPR}
}


@proceedings{IEEEInternationalConferenceonAutomaticFaceandGestureRecognition2018,
 year = {2018},
 title = {{13th IEEE International Conference on Automatic Face and Gesture Recognition: FG 2018 : 15-19 May 2018, Xi'an, China : proceedings}},
 keywords = {Gesicht;Mustererkennung},
 address = {Piscataway, NJ},
 publisher = {IEEE},
 isbn = {978-1-5386-2335-0},
 institution = {{IEEE International Conference on Automatic Face and Gesture Recognition} and {Institute of Electrical and Electronics Engineers} and FG}
}


@proceedings{InstituteofElectricalandElectronicsEngineers2013,
 year = {2013},
 title = {{IEEE Conference on Computer Vision and Pattern Recognition (CVPR), 2013: 23 - 28 June 2013, Portland, Oregon, USA ; proceedings}},
 keywords = {Maschinelles Sehen;Mustererkennung},
 address = {Piscataway, NJ},
 publisher = {IEEE},
 isbn = {978-0-7695-4989-7},
 institution = {{Institute of Electrical and Electronics Engineers} and {IEEE Computer Society} and {IEEE Conference on Computer Vision and Pattern Recognition} and CVPR}
}


@proceedings{InstituteofElectricalandElectronicsEngineers2015,
 year = {2015},
 title = {{2015 IEEE Conference on Computer Vision and Pattern Recognition (CVPR): - 12 June 2015, Boston, MA}},
 keywords = {Computer vision;Congresses;Image processing;Maschinelles Sehen;Mustererkennung;Optical pattern recognition;Pattern recognition systems},
 address = {Piscataway, NJ},
 publisher = {IEEE},
 isbn = {978-1-4673-6964-0},
 institution = {{Institute of Electrical and Electronics Engineers} and {IEEE Conference on Computer Vision and Pattern Recognition} and CVPR}
}


@proceedings{InstituteofElectricalandElectronicsEngineers2015b,
 year = {2015},
 title = {{2015 11th IEEE International Conference and Workshops on Automatic Face and Gesture Recognition (FG): 4 - 8 May 2015, Ljubljana, Slovenia}},
 keywords = {Gesicht;Mustererkennung},
 address = {Piscataway, NJ},
 publisher = {IEEE},
 isbn = {978-1-4799-6026-2},
 institution = {{Institute of Electrical and Electronics Engineers} and {IEEE Computer Society} and {IEEE Biometrics Council} and {IEEE International Conference on Automatic Face and Gesture Recognition (FG)} and {Workshop on De-Identification for Privacy Protection in Multimedia (DEID)} and {Facial Expression Recognition and Analysis Challenge and Workshop (FERA)} and {International Workshop on Context Based Affect Recognition (CBAR)} and {International Workshop on Biometrics is the Wild (B-Wild)} and {International Workshop on Understanding Human Activities through 3D Sensors (UHA3DS)} and {International Workshop on Emotion Representation, Analysis, and Synthesis in Continuous Time and Space (EmoSPACE)}}
}


@article{Jones2002,
 abstract = {The existence of large image datasets such as the set of photos on the World Wide Web make it

possible to build powerful generic models for low-level image attributes like color using simple histogram learning

techniques. We describe the construction of color models for skin and non-skin classes from a dataset of nearly

1 billion labelled pixels. These classes exhibit a surprising degree of separability which we exploit by building a skin

pixel detector achieving a detection rate of 80{\%} with 8.5{\%} false positives. We compare the performance of histogram

and mixture models in skin detection and ﬁnd histogram models to be superior in accuracy and computational cost.

Using aggregate features computed from the skin pixel detector we build a surprisingly effective detector for naked

people. Our results suggest that color can be a more powerful cue for detecting people in unconstrained imagery

than was previously suspected. We believe this work is the most comprehensive and detailed exploration of skin

color models to date.},
 author = {Jones, Michael J. and Rehg, James M.},
 year = {2002},
 title = {{Statistical Color Models with Application to Skin Detection}},
 pages = {81--96},
 volume = {46},
 number = {1},
 issn = {0920-5691},
 journal = {{International Journal of Computer Vision}},
 doi = {10.1023/A:1013200319198}
}


@book{Kang,
 author = {Kang, Sing Bing and Szeliski, Richard},
 title = {{Extracting View-Dependent Depth Maps from a Collection of Images}},
 keywords = {Artificial Intelligence (incl. Robotics);Computer Imaging, Vision, Pattern Recognition and Graphics;Image Processing and Computer Vision;Pattern Recognition},
 institution = {{SpringerLink (Online service)}}
}


@misc{Kang2016,
 abstract = {Hand segmentation for hand-object interaction is a necessary preprocessing step in many applications such as augmented reality, medical application, and human-robot interaction. However, typical methods are based on color information which is not robust to objects with skin color, skin pigment difference, and light condition variations. Thus, we propose hand segmentation method for hand-object interaction using only a depth map. It is challenging because of the small depth difference between a hand and objects during an interaction. To overcome this challenge, we propose the two-stage random decision forest (RDF) method consisting of detecting hands and segmenting hands. To validate the proposed method, we demonstrate results on the publicly available dataset of hand segmentation for hand-object interaction. The proposed method achieves high accuracy in short processing time comparing to the other state-of-the-art methods.},
 author = {Kang, Byeongkeun and Tan, Kar-Han and Jiang, Nan and Tai, Hung-Shuo and Tretter, Daniel and Nguyen, Truong Q.},
 date = {08.03.2016},
 title = {{Hand Segmentation for Hand-Object Interaction from Depth map}},
 url = {http://arxiv.org/pdf/1603.02345v3},
 keywords = {Dataset}
}


@incollection{Keskin2012,
 crossref = {Fitzgibbon2012},
 author = {Keskin, Cem and K{\i}ra{\c{c}}, Furkan and Kara, Yunus Emre and Akarun, Lale},
 title = {{Hand Pose Estimation and Hand Shape Classification Using Multi-layered Randomized Decision Forests}},
 pages = {852--863},
 volume = {7577},
 publisher = {Springer},
 isbn = {978-3-642-33782-6},
 series = {{Lecture Notes in Computer Science}},
 editor = {Fitzgibbon, Andrew and Lazebnik, Svetlana and Perona, Pietro and Sato, Yoichi and Schmid, Cordelia},
 booktitle = {{Computer vision - ECCV 2012}},
 year = {2012},
 address = {Berlin},
 doi = {10.1007/978-3-642-33783-3{\textunderscore }61}
}


@misc{Kopuklu2019,
 abstract = {Real-time recognition of dynamic hand gestures from video streams is a challenging task since (i) there is no indication when a gesture starts and ends in the video, (ii) performed gestures should only be recognized once, and (iii) the entire architecture should be designed considering the memory and power budget. In this work, we address these challenges by proposing a hierarchical structure enabling offline-working convolutional neural network (CNN) architectures to operate online efficiently by using sliding window approach. The proposed architecture consists of two models: (1) A detector which is a lightweight CNN architecture to detect gestures and (2) a classifier which is a deep CNN to classify the detected gestures. In order to evaluate the single-time activations of the detected gestures, we propose to use Levenshtein distance as an evaluation metric since it can measure misclassifications, multiple detections, and missing detections at the same time. We evaluate our architecture on two publicly available datasets - EgoGesture and NVIDIA Dynamic Hand Gesture Datasets - which require temporal detection and classification of the performed hand gestures. ResNeXt-101 model, which is used as a classifier, achieves the state-of-the-art offline classification accuracy of 94.04{\%} and 83.82{\%} for depth modality on EgoGesture and NVIDIA benchmarks, respectively. In real-time detection and classification, we obtain considerable early detections while achieving performances close to offline operation. The codes and pretrained models used in this work are publicly available.},
 author = {K{\"o}p{\"u}kl{\"u}, Okan and Gunduz, Ahmet and Kose, Neslihan and Rigoll, Gerhard},
 date = {1/29/2019},
 title = {{Real-time Hand Gesture Detection and Classification Using Convolutional  Neural Networks}},
 url = {http://arxiv.org/pdf/1901.10323v2},
 keywords = {Computer Science - Artificial Intelligence;Computer Science - Computer Vision and Pattern Recognition}
}


@incollection{Koza1996,
 crossref = {Gero1996},
 abstract = {This paper describes an automated process for designing analog electrical circuits based on the principles of natural selection, sexual recombination, and developmental biology. The design process starts with the random creation of a large population of program trees composed of circuit-constructing functions. Each program tree specifies the steps by which a fully developed circuit is to be progressively developed from a common embryonic circuit appropriate for the type of circuit that the user wishes to design. The fitness measure is a user-written computer program that may incorporate any calculable characteristic or combination of characteristics of the circuit. The population of program trees is genetically bred over a series of many generations using genetic programming. Genetic programming is driven by a fitness measure and employs genetic operations such as Darwinian reproduction, sexual recombination (crossover), and occasional mutation to create offspring. This automated evolutionary process produces both the topology of the circuit and the numerical values for each component. This paper describes how genetic programming can evolve the circuit for a difficult-to-design low-pass filter.},
 author = {Koza, John R. and Bennett, Forrest H. and Andre, David and Keane, Martin A.},
 title = {{Automated Design of Both the Topology and Sizing of Analog Electrical Circuits Using Genetic Programming}},
 pages = {151--170},
 publisher = {{Springer Netherlands}},
 isbn = {978-94-009-0279-4},
 editor = {Gero, John S. and Sudweeks, Fay},
 booktitle = {{Artificial Intelligence in Design '96}},
 year = {1996},
 address = {Dordrecht},
 doi = {10.1007/978-94-009-0279-4{\textunderscore }9}
}


@incollection{Koza1996b,
 crossref = {Gero1996},
 abstract = {This paper describes an automated process for designing analog electrical circuits based on the principles of natural selection, sexual recombination, and developmental biology. The design process starts with the random creation of a large population of program trees composed of circuit-constructing functions. Each program tree specifies the steps by which a fully developed circuit is to be progressively developed from a common embryonic circuit appropriate for the type of circuit that the user wishes to design. The fitness measure is a user-written computer program that may incorporate any calculable characteristic or combination of characteristics of the circuit. The population of program trees is genetically bred over a series of many generations using genetic programming. Genetic programming is driven by a fitness measure and employs genetic operations such as Darwinian reproduction, sexual recombination (crossover), and occasional mutation to create offspring. This automated evolutionary process produces both the topology of the circuit and the numerical values for each component. This paper describes how genetic programming can evolve the circuit for a difficult-to-design low-pass filter.},
 author = {Koza, John R. and Bennett, Forrest H. and Andre, David and Keane, Martin A.},
 title = {{Automated Design of Both the Topology and Sizing of Analog Electrical Circuits Using Genetic Programming}},
 pages = {151--170},
 publisher = {{Springer Netherlands}},
 isbn = {978-94-009-0279-4},
 editor = {Gero, John S. and Sudweeks, Fay},
 booktitle = {{Artificial Intelligence in Design '96}},
 year = {1996},
 address = {Dordrecht},
 doi = {10.1007/978-94-009-0279-4{\textunderscore }9}
}


@article{LeCun1989,
 author = {LeCun, Y. and Boser, B. and Denker, J. S. and Henderson, D. and Howard, R. E. and Hubbard, W. and Jackel, L. D.},
 year = {1989},
 title = {{Backpropagation Applied to Handwritten Zip Code Recognition}},
 pages = {541--551},
 volume = {1},
 number = {4},
 issn = {0899-7667},
 journal = {{Neural Computation}},
 doi = {10.1162/neco.1989.1.4.541}
}


@inproceedings{Li2013,
 crossref = {InstituteofElectricalandElectronicsEngineers2013},
 author = {Li, Cheng and Kitani, Kris M.},
 title = {{Pixel-Level Hand Detection in Ego-centric Videos}},
 pages = {3570--3577},
 publisher = {IEEE},
 isbn = {978-0-7695-4989-7},
 booktitle = {{IEEE Conference on Computer Vision and Pattern Recognition (CVPR), 2013}},
 year = {2013},
 address = {Piscataway, NJ},
 doi = {10.1109/CVPR.2013.458}
}


@article{Li2018,
 abstract = {Information Sciences, 441 (2018) 66-78. doi:10.1016/j.ins.2018.02.024},
 author = {Li, Yuan and Wang, Xinggang and Liu, Wenyu and Feng, Bin},
 year = {2018},
 title = {{Deep attention network for joint hand gesture localization and recognition using static RGB-D images}},
 keywords = {CNN;Hand gesture localization and recognition;RGB-D images;Soft attention mechanism},
 pages = {66--78},
 volume = {441},
 issn = {00200255},
 journal = {{Information Sciences}},
 doi = {10.1016/j.ins.2018.02.024}
}


@article{Li2019,
 author = {Li, Yong and He, Zihang and Ye, Xiang and He, Zuguo and Han, Kangrong},
 year = {2019},
 title = {{Spatial temporal graph convolutional networks for skeleton-based dynamic hand gesture recognition}},
 pages = {404},
 volume = {2019},
 number = {1},
 journal = {{EURASIP Journal on Image and Video Processing}},
 doi = {10.1186/s13640-019-0476-x}
}


@inproceedings{Lin2000,
 crossref = {WorkshoponHumanMotion2000},
 author = {Lin, John and Wu, Ying and Huang, T. S.},
 title = {{Modeling the constraints of human hand motion}},
 pages = {121--126},
 publisher = {{IEEE Computer Society}},
 isbn = {0-7695-0939-8},
 booktitle = {{Proceedings, Workshop on Human Motion}},
 year = {2000},
 address = {Los Alamitos, Calif},
 doi = {10.1109/HUMO.2000.897381}
}


@article{Liu2016,
 author = {Liu, Zhi and Zhang, Chenyang and Tian, Yingli},
 year = {2016},
 title = {{3D-based Deep Convolutional Neural Network for action recognition with depth sequences}},
 pages = {93--100},
 volume = {55},
 issn = {02628856},
 journal = {{Image and Vision Computing}},
 doi = {10.1016/j.imavis.2016.04.004}
}


@proceedings{Liu2017,
 year = {2017},
 title = {{MM'17: Proceedings of the 2017 ACM Multimedia Conference : October 23-27, 2017, Amsterdam, the Netherlands}},
 address = {New York, NY, USA},
 publisher = {{ACM Association for Computing Machinery}},
 isbn = {9781450349062},
 editor = {Liu, Qiong and Lienhart, Rainer and Wang, Haohong and Chen, Sheng-Wei {\textquotedbl}Kuan-Ta{\textquotedbl} and Boll, Susanne and Chen, Phoebe and Friedland, Gerald and Li, Jia and Yan, Shuicheng},
 institution = {{ACM Multimedia Conference} and {Association for Computing Machinery} and {ACM Multimedia} and MM},
 doi = {10.1145/3123266}
}


@proceedings{Lu2011,
 abstract = {Computer vision system is one of the newest approaches for human computer interaction. Recently, the direct use of our hands as natural input devices has shown promising progress. Toward this progress, we introduce a hand gesture recognition system in this study to recognize real time gesture in unconstrained environments. The system consists of three components: real time hand tracking, hand-tree construction, and hand gesture recognition. Our main contribution includes: (1) a simple way to represent the hand gesture after applying thinning algorithm to the image, and (2) using a model of complex-valued neural network (CVNN) for real-valued classification. We have tested our system to 26 different gestures to evaluate the effectiveness of our approach. The results show that the classification ability of single-layered CVNN on unseen data is comparable to the conventional real-valued neural network (RVNN) having one hidden layer. Moreover, convergence of the CVNN is much faster than that of the RVNN in most cases.},
 year = {2011},
 title = {{Real-Time Hand Gesture Recognition Using Complex-Valued Neural Network (CVNN): Neural Information Processing}},
 publisher = {{Springer Berlin Heidelberg}},
 isbn = {978-3-642-24955-6},
 editor = {Lu, Bao-Liang and Zhang, Liqing and Kwok, James and Hafiz, Abdul Rahman and Amin, Md. Faijul and Murase, Kazuyuki}
}


@misc{Maddison2014,
 abstract = {The game of Go is more challenging than other board games, due to the difficulty of constructing a position or move evaluation function. In this paper we investigate whether deep convolutional networks can be used to directly represent and learn this knowledge. We train a large 12-layer convolutional neural network by supervised learning from a database of human professional games. The network correctly predicts the expert move in 55{\%} of positions, equalling the accuracy of a 6 dan human player. When the trained convolutional network was used directly to play games of Go, without any search, it beat the traditional search program GnuGo in 97{\%} of games, and matched the performance of a state-of-the-art Monte-Carlo tree search that simulates a million positions per move.},
 author = {Maddison, Chris J. and Huang, Aja and Sutskever, Ilya and Silver, David},
 date = {12/20/2014},
 title = {{Move Evaluation in Go Using Deep Convolutional Neural Networks}},
 url = {http://arxiv.org/pdf/1412.6564v2},
 keywords = {Computer Science - Learning;Computer Science - Neural and Evolutionary Computing}
}


@misc{Malik2017,
 abstract = {Articulated hand pose estimation is a challenging task for human-computer interaction. The state-of-the-art hand pose estimation algorithms work only with one or a few subjects for which they have been calibrated or trained. Particularly, the hybrid methods based on learning followed by model fitting or model based deep learning do not explicitly consider varying hand shapes and sizes. In this work, we introduce a novel hybrid algorithm for estimating the 3D hand pose as well as bone-lengths of the hand skeleton at the same time, from a single depth image. The proposed CNN architecture learns hand pose parameters and scale parameters associated with the bone-lengths simultaneously. Subsequently, a new hybrid forward kinematics layer employs both parameters to estimate 3D joint positions of the hand. For end-to-end training, we combine three public datasets NYU, ICVL and MSRA-2015 in one unified format to achieve large variation in hand shapes and sizes. Among hybrid methods, our method shows improved accuracy over the state-of-the-art on the combined dataset and the ICVL dataset that contain multiple subjects. Also, our algorithm is demonstrated to work well with unseen images.},
 author = {Malik, Jameel and Elhayek, Ahmed and Stricker, Didier},
 date = {12/8/2017},
 title = {{Simultaneous Hand Pose and Skeleton Bone-Lengths Estimation from a  Single Depth Image}},
 url = {http://arxiv.org/pdf/1712.03121v1},
 keywords = {Computer Science - Computer Vision and Pattern Recognition;Computer Science - Human-Computer Interaction}
}


@misc{Malik2018,
 abstract = {Articulated hand pose and shape estimation is an important problem for vision-based applications such as augmented reality and animation. In contrast to the existing methods which optimize only for joint positions, we propose a fully supervised deep network which learns to jointly estimate a full 3D hand mesh representation and pose from a single depth image. To this end, a CNN architecture is employed to estimate parametric representations i.e. hand pose, bone scales and complex shape parameters. Then, a novel hand pose and shape layer, embedded inside our deep framework, produces 3D joint positions and hand mesh. Lack of sufficient training data with varying hand shapes limits the generalized performance of learning based methods. Also, manually annotating real data is suboptimal. Therefore, we present SynHand5M: a million-scale synthetic dataset with accurate joint annotations, segmentation masks and mesh files of depth maps. Among model based learning (hybrid) methods, we show improved results on our dataset and two of the public benchmarks i.e. NYU and ICVL. Also, by employing a joint training strategy with real and synthetic data, we recover 3D hand mesh and pose from real images in 3.7ms.},
 author = {Malik, Jameel and Elhayek, Ahmed and Nunnari, Fabrizio and Varanasi, Kiran and Tamaddon, Kiarash and Heloir, Alexis and Stricker, Didier},
 date = {8/28/2018},
 title = {{DeepHPS: End-to-end Estimation of 3D Hand Pose and Shape by Learning  from Synthetic Depth}},
 url = {http://arxiv.org/pdf/1808.09208v1},
 keywords = {Computer Science - Computer Vision and Pattern Recognition}
}


@incollection{Malik2018b,
 crossref = {Bourdot2018},
 author = {Malik, Jameel and Elhayek, Ahmed and Stricker, Didier},
 title = {{Structure-Aware 3D Hand Pose Regression from a Single Depth Image}},
 pages = {3--17},
 volume = {11162},
 publisher = {Springer},
 isbn = {978-3-030-01789-7},
 series = {{Lecture Notes in Computer Science}},
 editor = {Bourdot, Patrick and Cobb, Sue and Interrante, Victoria and kato, Hirokazu and Stricker, Didier},
 booktitle = {{Virtual reality and augmented reality}},
 year = {2018},
 address = {Cham},
 doi = {10.1007/978-3-030-01790-3{\textunderscore }1}
}


@inproceedings{Marstaller2019,
 crossref = {2019},
 author = {Marstaller, Julian and Tausch, Frederic and Stock, Simon},
 title = {{DeepBees - Building and Scaling Convolutional Neuronal Nets For Fast and Large-Scale Visual Monitoring of Bee Hives}},
 url = {http://openaccess.thecvf.com/content_ICCVW_2019/papers/CVWC/Marstaller_DeepBees_-_Building_and_Scaling_Convolutional_Neuronal_Nets_For_Fast_ICCVW_2019_paper.pdf},
 pages = {0},
 year = {2019}
}


@article{McCulloch1943,
 author = {McCulloch, Warren S. and Pitts, Walter},
 year = {1943},
 title = {{A logical calculus of the ideas immanent in nervous activity}},
 pages = {115--133},
 volume = {5},
 number = {4},
 issn = {0007-4985},
 journal = {{The Bulletin of Mathematical Biophysics}},
 doi = {10.1007/BF02478259}
}


@misc{Melax2017,
 abstract = {Tracking the full skeletal pose of the hands and fingers is a challenging problem that has a plethora of applications for user interaction. Existing techniques either require wearable hardware, add restrictions to user pose, or require significant computation resources. This research explores a new approach to tracking hands, or any articulated model, by using an augmented rigid body simulation. This allows us to phrase 3D object tracking as a linear complementarity problem with a well-defined solution. Based on a depth sensor's samples, the system generates constraints that limit motion orthogonal to the rigid body model's surface. These constraints, along with prior motion, collision/contact constraints, and joint mechanics, are resolved with a projected Gauss-Seidel solver. Due to camera noise properties and attachment errors, the numerous surface constraints are impulse capped to avoid overpowering mechanical constraints. To improve tracking accuracy, multiple simulations are spawned at each frame and fed a variety of heuristics, constraints and poses. A 3D error metric selects the best-fit simulation, helping the system handle challenging hand motions. Such an approach enables real-time, robust, and accurate 3D skeletal tracking of a user's hand on a variety of depth cameras, while only utilizing a single x86 CPU core for processing.},
 author = {Melax, Stan and Keselman, Leonid and Orsten, Sterling},
 date = {5/22/2017},
 title = {{Dynamics Based 3D Skeletal Hand Tracking}},
 url = {http://arxiv.org/pdf/1705.07640v1},
 keywords = {Computer Science - Computer Vision and Pattern Recognition;Computer Science - Graphics}
}


@book{Mohri2018,
 author = {Mohri, Mehryar and Rostamizadeh, Afshin and Talwalkar, Ameet},
 year = {2018},
 title = {{Foundations of machine learning}},
 edition = {Second edition},
 isbn = {0262039400},
 series = {{Adaptive computation and machine learning}}
}


@inproceedings{Molchanov2016,
 crossref = {2016},
 abstract = {2016 IEEE Conference on Computer Vision and Pattern Recognition (CVPR);2016; ; ;10.1109/CVPR.2016.456},
 author = {Molchanov, Pavlo and Yang, Xiaodong and Gupta, Shalini and Kim, Kihwan and Tyree, Stephen and Kautz, Jan},
 title = {{Online Detection and Classification of Dynamic Hand Gestures with Recurrent 3D Convolutional Neural Networks}},
 keywords = {Nvidia},
 pages = {4207--4215},
 publisher = {IEEE},
 isbn = {978-1-4673-8851-1},
 booktitle = {{2016 IEEE Conference on Computer Vision and Pattern Recognition (CVPR)}},
 year = {27.06.2016 - 30.06.2016},
 doi = {10.1109/CVPR.2016.456}
}


@article{Mueller2016,
 author = {Mueller, Jonas and Thyagarajan, Aditya},
 year = {2016},
 title = {{Siamese Recurrent Architectures for Learning Sentence Similarity}},
 url = {https://pdfs.semanticscholar.org/72b8/9e45e8ad8b44bdcab524b959dc09bf63eb1e.pdf},
 journal = {undefined}
}


@misc{Murray2019,
 author = {Murray, John},
 editor = {towardsdatascience.com},
 year = {24.04.2019},
 title = {{Racist Data? Human Bias is Infecting AI Development}},
 url = {https://towardsdatascience.com/racist-data-human-bias-is-infecting-ai-development-8110c1ec50c},
 urldate = {11.12.2019}
}


@misc{Newell2016,
 author = {Newell, Alejandro and Yang, Kaiyu and Deng, Jia},
 year = {2016},
 title = {{Stacked Hourglass Networks for Human Pose Estimation}},
 url = {https://arxiv.org/pdf/1603.06937}
}


@article{Nunez2018,
 abstract = {Pattern Recognition, 76 (2017) 80-94. doi:10.1016/j.patcog.2017.10.033},
 author = {N{\'u}{\~n}ez, Juan C. and Cabido, Ra{\'u}l and Pantrigo, Juan J. and Montemayor, Antonio S. and V{\'e}lez, Jos{\'e} F.},
 year = {2018},
 title = {{Convolutional Neural Networks and Long Short-Term Memory for skeleton-based human activity and hand gesture recognition}},
 keywords = {Convolutional Neural Network;Deep learning;Hand gesture recognition;Human activity recognition;Long Short-Term Memory;Real-time;Recurrent neural network},
 pages = {80--94},
 volume = {76},
 issn = {00313203},
 journal = {{Pattern Recognition}},
 doi = {10.1016/j.patcog.2017.10.033}
}


@misc{Oberweger2016,
 author = {Oberweger, Markus and Wohlhart, Paul and Lepetit, Vincent},
 year = {2015},
 title = {{Hands Deep in Deep Learning for Hand Pose Estimation}},
 url = {https://arxiv.org/pdf/1502.06807}
}


@article{OhnBar2014,
 author = {Ohn-Bar, Eshed and Trivedi, Mohan Manubhai},
 year = {2014},
 title = {{Hand Gesture Recognition in Real Time for Automotive Interfaces: A Multimodal Vision-Based Approach and Evaluations}},
 pages = {2368--2377},
 volume = {15},
 number = {6},
 issn = {1524-9050},
 journal = {{IEEE Transactions on Intelligent Transportation Systems}},
 doi = {10.1109/TITS.2014.2337331}
}


@inproceedings{Oikonomidis2011,
 crossref = {Hoey2011},
 author = {Oikonomidis, Iason and Kyriazis, Nikolaos and Argyros, Antonis},
 title = {{Efficient model-based 3D tracking of hand articulations using Kinect}},
 pages = {101.1--101.11},
 publisher = {{BMVA Press}},
 isbn = {1-901725-43-X},
 editor = {Hoey, Jesse},
 booktitle = {{Proceedings of the British Machine Vision Conference 2011}},
 year = {2011},
 address = {Durham},
 doi = {10.5244/C.25.101}
}


@article{Omohundro1989,
 author = {Omohundro, Stephen M.},
 year = {1989},
 title = {{Five Balltree Construction Algorithms}},
 url = {https://pdfs.semanticscholar.org/17ac/002939f8e950ffb32ec4dc8e86bdd8cb5ff1.pdf}
}


@article{Patrona2018,
 abstract = {Pattern Recognition, 76 (2017) 612-622. doi:10.1016/j.patcog.2017.12.007},
 author = {Patrona, Fotini and Chatzitofis, Anargyros and Zarpalas, Dimitrios and Daras, Petros},
 year = {2018},
 title = {{Motion analysis: Action detection, recognition and evaluation based on motion capture data}},
 keywords = {Automatic joint/angle weighting;Kinect;Kinetic energy;Motion evaluation;Online human action detection;Online human action recognition;Skeleton data},
 pages = {612--622},
 volume = {76},
 issn = {00313203},
 journal = {{Pattern Recognition}},
 doi = {10.1016/j.patcog.2017.12.007}
}


@article{Pereira2019,
 abstract = {The need for automated and efficient systems for tracking full animal pose has increased with the complexity of behavioral data and analyses. Here we introduce LEAP (LEAP estimates animal pose), a deep-learning-based method for predicting the positions of animal body parts. This framework consists of a graphical interface for labeling of body parts and training the network. LEAP offers fast prediction on new data, and training with as few as 100 frames results in 95{\%} of peak performance. We validated LEAP using videos of freely behaving fruit flies and tracked 32 distinct points to describe the pose of the head, body, wings and legs, with an error rate of {\textless}3{\%} of body length. We recapitulated reported findings on insect gait dynamics and demonstrated LEAP's applicability for unsupervised behavioral classification. Finally, we extended the method to more challenging imaging situations and videos of freely moving mice.},
 author = {Pereira, Talmo D. and Aldarondo, Diego E. and Willmore, Lindsay and Kislin, Mikhail and Wang, Samuel S-H and Murthy, Mala and Shaevitz, Joshua W.},
 year = {2019},
 title = {{Fast animal pose estimation using deep neural networks}},
 url = {https://www.nature.com/articles/s41592-018-0234-5.pdf},
 pages = {117--125},
 volume = {16},
 number = {1},
 issn = {1548-7105},
 journal = {{Nature Methods}},
 doi = {10.1038/s41592-018-0234-5}
}


@incollection{Pfister2015,
 crossref = {Cremers2015},
 author = {Pfister, Tomas and Simonyan, Karen and Charles, James and Zisserman, Andrew},
 title = {{Deep Convolutional Neural Networks for Efficient Pose Estimation in Gesture Videos}},
 pages = {538--552},
 volume = {9003},
 publisher = {{Springer International Publishing}},
 isbn = {978-3-319-16864-7},
 series = {{Lecture Notes in Computer Science}},
 editor = {Cremers, Daniel and Reid, Ian and Saito, Hideo and Yang, Ming-Hsuan},
 booktitle = {{Computer Vision -- ACCV 2014}},
 year = {2015},
 address = {Cham},
 doi = {10.1007/978-3-319-16865-4{\textunderscore }35}
}


@inproceedings{Poier2015,
 crossref = {Xie2015},
 author = {Poier, Georg and Roditakis, Konstantinos and Schulter, Samuel and Michel, Damien and Bischof, Horst and Argyros, Antonis A.},
 title = {{Hybrid One-Shot 3D Hand Pose Estimation by Exploiting Uncertainties}},
 pages = {182.1--182.14},
 publisher = {{BMVA Press}},
 isbn = {1-901725-53-7},
 editor = {Xie, Xianghua and Jones, Mark W. and Tam, Gary K. L.},
 booktitle = {{Proceedings of the British Machine Vision Conference 2015}},
 year = {2015},
 address = {Durham},
 doi = {10.5244/C.29.182}
}


@misc{Redmon2016,
 author = {Redmon, Joseph and Divvala, Santosh and Girshick, Ross and Farhadi, Ali},
 year = {2015},
 title = {{You Only Look Once: Unified, Real-Time Object Detection}},
 url = {https://arxiv.org/pdf/1506.02640}
}


@article{ROSENBLATT1958,
 author = {ROSENBLATT, F.},
 year = {1958},
 title = {{The perceptron: a probabilistic model for information storage and organization in the brain}},
 keywords = {Brain;Humans;Information Storage and Retrieval;Models, Statistical;Neural Networks (Computer);Perception},
 pages = {386--408},
 volume = {65},
 number = {6},
 issn = {0033-295X},
 journal = {{Psychological review}},
 doi = {10.1037/h0042519}
}


@misc{ROSENBLATT1961,
 author = {ROSENBLATT, FRANK},
 date = {1961},
 title = {{PRINCIPLES OF NEURODYNAMICS. PERCEPTRONS AND THE THEORY OF BRAIN MECHANISMS}},
 url = {https://apps.dtic.mil/dtic/tr/fulltext/u2/256582.pdf},
 keywords = {*CYBERNETICS;*NERVOUS SYSTEM;*PERCEPTION;Brain;COMPUTERS;MATHEMATICAL ANALYSIS;PSYCHOLOGY;THEORY},
 number = {VG-1196-G-8},
 series = {{CORNELL AERONAUTICAL LAB INC BUFFALO NY}}
}


@book{Russell2016,
 author = {Russell, Stuart J. and Norvig, Peter},
 year = {2016},
 title = {{Artificial intelligence: A modern approach}},
 address = {Boston and Columbus and Indianapolis},
 edition = {Third edition, Global edition},
 publisher = {Pearson},
 isbn = {9780136042594},
 series = {{Always learning}}
}


@book{Russell2016b,
 author = {Russell, Stuart J. and Norvig, Peter and Davis, Ernest and Edwards, Douglas},
 year = {2016},
 title = {{Artificial intelligence: A modern approach}},
 address = {Boston and Columbus and Indianapolis and New York and San Francisco and Upper Saddle River and Amsterdam, Cape Town and Dubai and London and Madrid and Milan and Munich and Paris and Montreal and Toronto and Delhi and Mexico City and Sao Paulo and Sydney and Hong Kong and Seoul and Singapore and Taipei and Tokyo},
 edition = {Third edition, Global edition},
 publisher = {Pearson},
 isbn = {9781292153964},
 series = {{Always learning}}
}


@article{Samuel1959,
 author = {Samuel, A. L.},
 year = {1959},
 title = {{Some Studies in Machine Learning Using the Game of Checkers}},
 pages = {210--229},
 volume = {3},
 number = {3},
 issn = {0018-8646},
 journal = {{IBM Journal of Research and Development}},
 doi = {10.1147/rd.33.0210}
}


@article{Sandler,
 abstract = {In this paper we describe a new mobile architecture, MobileNetV2, that improves the state of the art performance of mobile models on multiple tasks and benchmarks as well as across a spectrum of different model sizes. We also describe efficient ways of applying these mobile models to object detection in a novel framework we call SSDLite. Additionally, we demonstrate how to build mobile semantic segmentation models through a reduced form of DeepLabv3 which we call Mobile DeepLabv3.  The MobileNetV2 architecture is based on an inverted residual structure where the input and output of the residual block are thin bottleneck layers opposite to traditional residual models which use expanded representations in the input an MobileNetV2 uses lightweight depthwise convolutions to filter features in the intermediate expansion layer. Additionally, we find that it is important to remove non-linearities in the narrow layers in order to maintain representational power. We demonstrate that this improves performance and provide an intuition that led to this design. Finally, our approach allows decoupling of the input/output domains from the expressiveness of the transformation, which provides a convenient framework for further analysis. We measure our performance on Imagenet classification, COCO object detection, VOC image segmentation. We evaluate the trade-offs between accuracy, and number of operations measured by multiply-adds (MAdd), as well as the number of parameters},
 author = {Sandler, Mark and Howard, Andrew and Zhu, Menglong and Zhmoginov, Andrey and Chen, Liang-Chieh},
 title = {{MobileNetV2: Inverted Residuals and Linear Bottlenecks}},
 url = {http://arxiv.org/pdf/1801.04381v4},
 keywords = {Computer Science - Computer Vision and Pattern Recognition}
}


@inproceedings{Sato2001,
 crossref = {2001},
 abstract = {Proceedings IEEE Virtual Reality 2001;2001; ; ;10.1109/VR.2001.913773},
 author = {Sato, Y. and Saito, M. and Koike, H.},
 title = {{Real-time input of 3D pose and gestures of a user's hand and its applications for HCI}},
 pages = {79--86},
 publisher = {{IEEE Comput. Soc}},
 isbn = {0-7695-0948-7},
 booktitle = {{Proceedings IEEE Virtual Reality 2001}},
 year = {13-17 March 2001},
 doi = {10.1109/VR.2001.913773}
}


@book{Scharstein,
 author = {Scharstein, Daniel and Szeliski, Richard},
 title = {{Stereo Matching with Nonlinear Diffusion}},
 keywords = {Artificial Intelligence (incl. Robotics);Computer Imaging, Vision, Pattern Recognition and Graphics;Image Processing and Computer Vision;Pattern Recognition},
 institution = {{SpringerLink (Online service)}}
}


@misc{Sermanet2013,
 abstract = {We present an integrated framework for using Convolutional Networks for classification, localization and detection. We show how a multiscale and sliding window approach can be efficiently implemented within a ConvNet. We also introduce a novel deep learning approach to localization by learning to predict object boundaries. Bounding boxes are then accumulated rather than suppressed in order to increase detection confidence. We show that different tasks can be learned simultaneously using a single shared network. This integrated framework is the winner of the localization task of the ImageNet Large Scale Visual Recognition Challenge 2013 (ILSVRC2013) and obtained very competitive results for the detection and classifications tasks. In post-competition work, we establish a new state of the art for the detection task. Finally, we release a feature extractor from our best model called OverFeat.},
 author = {Sermanet, Pierre and Eigen, David and Zhang, Xiang and Mathieu, Michael and Fergus, Rob and Lecun, Yann},
 date = {12/21/2013},
 title = {{OverFeat: Integrated Recognition, Localization and Detection using  Convolutional Networks}},
 url = {http://arxiv.org/pdf/1312.6229v4},
 keywords = {Computer Science - Computer Vision and Pattern Recognition}
}


@misc{Shotton2011,
 author = {Shotton, Jamie and Fitzgibbon, Andrew and Blake, Andrew and Kipman, Alex and Finocchio, Mark and Moore, Bob and Sharp, Toby},
 year = {2011},
 title = {{Real-Time Human Pose Recognition in Parts from a Single Depth Image}},
 url = {https://www.microsoft.com/en-us/research/wp-content/uploads/2016/02/BodyPartRecognition.pdf}
}


@misc{Simon2017,
 abstract = {We present an approach that uses a multi-camera system to train fine-grained detectors for keypoints that are prone to occlusion, such as the joints of a hand. We call this procedure multiview bootstrapping: first, an initial keypoint detector is used to produce noisy labels in multiple views of the hand. The noisy detections are then triangulated in 3D using multiview geometry or marked as outliers. Finally, the reprojected triangulations are used as new labeled training data to improve the detector. We repeat this process, generating more labeled data in each iteration. We derive a result analytically relating the minimum number of views to achieve target true and false positive rates for a given detector. The method is used to train a hand keypoint detector for single images. The resulting keypoint detector runs in realtime on RGB images and has accuracy comparable to methods that use depth sensors. The single view detector, triangulated over multiple views, enables 3D markerless hand motion capture with complex object interactions.},
 author = {Simon, Tomas and Joo, Hanbyul and Matthews, Iain and Sheikh, Yaser},
 date = {25.04.2017},
 title = {{Hand Keypoint Detection in Single Images using Multiview Bootstrapping}},
 url = {http://arxiv.org/pdf/1704.07809v1}
}


@misc{Solly2019,
 author = {Solly, Meilan},
 editor = {{Smithsonian Institution}},
 year = {24.09.2019},
 title = {{Art Project Shows Racial Biases in Artificial Intelligence System}},
 url = {https://www.smithsonianmag.com/smart-news/art-project-exposed-racial-biases-artificial-intelligence-system-180973207/},
 urldate = {11.12.2019}
}


@inproceedings{Sridhar2013,
 crossref = {2013},
 author = {Sridhar, Srinath and Oulasvirta, Antti and Theobalt, Christian},
 title = {{Interactive Markerless Articulated Hand Motion Tracking using RGB and Depth Data}},
 url = {http://handtracker.mpi-inf.mpg.de/projects/handtracker_iccv2013/},
 booktitle = {{Proceedings of the IEEE International Conference on Computer Vision (ICCV)}},
 year = {2013}
}


@phdthesis{Stark,
 author = {Stark, Michael},
 title = {{On knowledge transfer in object class recognition}},
 type = {{Darmstadt, Techn. Univ., Diss., 2010}}
}


@inproceedings{Sun2015,
 crossref = {InstituteofElectricalandElectronicsEngineers2015},
 author = {Sun, Xiao and Wei, Yichen and Liang, Shuang and Tang, Xiaoou and Sun, Jian},
 title = {{Cascaded hand pose regression}},
 pages = {824--832},
 publisher = {IEEE},
 isbn = {978-1-4673-6964-0},
 booktitle = {{2015 IEEE Conference on Computer Vision and Pattern Recognition (CVPR)}},
 year = {2015},
 address = {Piscataway, NJ},
 doi = {10.1109/CVPR.2015.7298683}
}


@article{Supancic2018,
 author = {Supan{\v{c}}i{\v{c}}, James Steven and Rogez, Gr{\'e}gory and Yang, Yi and Shotton, Jamie and Ramanan, Deva},
 year = {2018},
 title = {{Depth-Based Hand Pose Estimation: Methods, Data, and Challenges}},
 pages = {1180--1198},
 volume = {126},
 number = {11},
 issn = {0920-5691},
 journal = {{International Journal of Computer Vision}},
 doi = {10.1007/s11263-018-1081-7}
}


@misc{Szegedy2014,
 abstract = {We propose a deep convolutional neural network architecture codenamed {\textquotedbl}Inception{\textquotedbl}, which was responsible for setting the new state of the art for classification and detection in the ImageNet Large-Scale Visual Recognition Challenge 2014 (ILSVRC 2014). The main hallmark of this architecture is the improved utilization of the computing resources inside the network. This was achieved by a carefully crafted design that allows for increasing the depth and width of the network while keeping the computational budget constant. To optimize quality, the architectural decisions were based on the Hebbian principle and the intuition of multi-scale processing. One particular incarnation used in our submission for ILSVRC 2014 is called GoogLeNet, a 22 layers deep network, the quality of which is assessed in the context of classification and detection.},
 author = {Szegedy, Christian and Liu, Wei and Jia, Yangqing and Sermanet, Pierre and Reed, Scott and Anguelov, Dragomir and Erhan, Dumitru and Vanhoucke, Vincent and Rabinovich, Andrew},
 date = {17.09.2014},
 title = {{Going Deeper with Convolutions}},
 url = {http://arxiv.org/pdf/1409.4842v1}
}


@book{Szeliski1989,
 author = {Szeliski, Richard},
 year = {1989},
 title = {{Bayesian modeling of uncertainty in low-level vision}},
 keywords = {Bayes-Verfahren;Bildverarbeitung},
 isbn = {0792390393},
 series = {{The Kluwer international series in engineering and computer science}}
}


@book{Szeliski2006,
 author = {Szeliski, Richard},
 year = {2006},
 title = {{Image alignment and stitching: A Tutorial}},
 keywords = {Algorithmus;Bildverarbeitung},
 address = {Hanover Mass. u.a.},
 volume = {2,1},
 publisher = {{Now Publ}},
 isbn = {1-933019-04-2},
 series = {{Foundations and trends in computer graphics and vision}}
}


@book{Szeliski2010,
 author = {Szeliski, Richard},
 year = {2010},
 title = {{Computer Vision: Algorithms and Applications}},
 address = {Guildford, Surrey},
 edition = {1. Aufl.},
 publisher = {{Springer London}},
 isbn = {9781848829343},
 series = {{Texts in computer science}}
}


@book{Szeliski2011,
 author = {Szeliski, Richard},
 year = {2011},
 title = {{Computer vision: Algorithms and applications}},
 price = {EUR 80.20},
 keywords = {Bildverarbeitung;Lehrbuch;Maschinelles Sehen},
 isbn = {9781848829343},
 series = {{Texts in computer science}}
}


@book{Szeliski2011b,
 author = {Szeliski, Richard},
 year = {2011},
 title = {{Computer Vision: Algorithms and Applications}},
 address = {London},
 publisher = {{Springer London}},
 isbn = {9781848829350},
 series = {{Texts in computer science}}
}


@inproceedings{Tang2014,
 crossref = {2014},
 author = {Tang, Danhang and Yu, Tsz-Ho and Kim, Tae-Kyun},
 title = {{Real-Time Articulated Hand Pose Estimation Using Semi-supervised Transductive Regression Forests}},
 pages = {3224--3231},
 publisher = {IEEE},
 isbn = {978-1-4799-2840-8},
 booktitle = {{2013 IEEE International Conference on Computer Vision}},
 year = {2014},
 address = {New York},
 doi = {10.1109/ICCV.2013.400}
}


@article{Tompson2014,
 author = {Tompson, Jonathan and Stein, Murphy and Lecun, Yann and Perlin, Ken},
 year = {2014},
 title = {{Real-Time Continuous Pose Recovery of Human Hands Using Convolutional Networks}},
 pages = {1--10},
 volume = {33},
 number = {5},
 issn = {07300301},
 journal = {{ACM Transactions on Graphics}},
 doi = {10.1145/2629500}
}


@misc{UniFreiburg,
 author = {{Uni Freiburg}},
 title = {{Rendered Handpose Dataset}},
 url = {https://lmb.informatik.uni-freiburg.de/resources/datasets/RenderedHandposeDataset.en.html},
 publisher = {{Uni Freiburg}}
}


@proceedings{Unknown2018,
 year = {2018},
 title = {{Proceedings of the 2018 International Conference on Machine Learning and Machine Intelligence  - MLMI2018}},
 address = {New York, New York, USA},
 publisher = {{ACM Press}},
 isbn = {9781450365567},
 editor = {Unknown},
 doi = {10.1145/3278312}
}


@book{Villa2016,
 year = {2016},
 title = {{Artificial neural networks and machine learning - ICANN 2016: 25th International Conference on Artificial Networks, Barcelona, Spain, September 6-9, 2016 ; proceedings}},
 address = {Switzerland},
 volume = {9887},
 publisher = {Springer},
 isbn = {978-3-319-44780-3},
 series = {{Lecture Notes in Computer Science}},
 editor = {Villa, Alessandro E.P. and Maulli, Paolo and {Pons Rivero}, Antonio Javier},
 doi = {10.1007/978-3-319-44781-0}
}


@article{Wang2016,
 author = {Wang, Pichao and Li, Wanqing and Gao, Zhimin and Zhang, Jing and Tang, Chang and Ogunbona, Philip O.},
 year = {2016},
 title = {{Action Recognition From Depth Maps Using Deep Convolutional Neural Networks}},
 pages = {498--509},
 volume = {46},
 number = {4},
 issn = {2168-2291},
 journal = {{IEEE Transactions on Human-Machine Systems}},
 doi = {10.1109/THMS.2015.2504550}
}


@proceedings{WorkshoponHumanMotion2000,
 year = {2000},
 title = {{Proceedings, Workshop on Human Motion: 7-8 December 2000, Austin, Texas}},
 keywords = {Automation;Computer simulation;Computer vision;Congresses;Data processing;Face perception;Gesture;Human locomotion;Kinesiology;Optical pattern recognition},
 address = {Los Alamitos, Calif},
 publisher = {{IEEE Computer Society}},
 isbn = {0-7695-0939-8},
 institution = {{Workshop on Human Motion} and {IEEE Computer Society}}
}


@article{Wu2016,
 author = {Wu, Chih-Hung and Chen, Wei-Lun and Lin, Chang Hong},
 year = {2016},
 title = {{Depth-based hand gesture recognition}},
 pages = {7065--7086},
 volume = {75},
 number = {12},
 issn = {1380-7501},
 journal = {{Multimedia Tools and Applications}},
 doi = {10.1007/s11042-015-2632-3}
}


@proceedings{Xie2015,
 year = {2015},
 title = {{Proceedings of the British Machine Vision Conference 2015: BMVC 2015, 7-10 September, Swansea, UK}},
 address = {Durham},
 publisher = {{BMVA Press}},
 isbn = {1-901725-53-7},
 editor = {Xie, Xianghua and Jones, Mark W. and Tam, Gary K. L.},
 institution = {{British Machine Vision Conference} and BMVC},
 doi = {10.5244/C.29}
}


@article{Xin2016,
 abstract = {Recognition of human actions from digital video is a challenging task due to complex interfering factors in uncontrolled realistic environments. In this paper, we propose a learning framework using static, dynamic and sequential mixed features to solve three fundamental problems: spatial domain variation, temporal domain polytrope, and intra- and inter-class diversities. Utilizing a cognitive-based data reduction method and a hybrid {\textquotedbl}network upon networks{\textquotedbl} architecture, we extract human action representations which are robust against spatial and temporal interferences and adaptive to variations in both action speed and duration. We evaluated our method on the UCF101 and other three challenging datasets. Our results demonstrated a superior performance of the proposed algorithm in human action recognition.},
 author = {Xin, Miao and Zhang, Hong and Wang, Helong and Sun, Mingui and Yuan, Ding},
 year = {2016},
 title = {{ARCH: Adaptive recurrent-convolutional hybrid networks for long-term action recognition}},
 keywords = {Action recognition;Deep learning;Hybrid feature learning},
 pages = {87--102},
 volume = {178},
 issn = {0925-2312},
 journal = {{Neurocomputing}},
 doi = {10.1016/j.neucom.2015.09.112}
}


@misc{Yuan2017,
 abstract = {In this paper we introduce a large-scale hand pose dataset, collected using a novel capture method. Existing datasets are either generated synthetically or captured using depth sensors: synthetic datasets exhibit a certain level of appearance difference from real depth images, and real datasets are limited in quantity and coverage, mainly due to the difficulty to annotate them. We propose a tracking system with six 6D magnetic sensors and inverse kinematics to automatically obtain 21-joints hand pose annotations of depth maps captured with minimal restriction on the range of motion. The capture protocol aims to fully cover the natural hand pose space. As shown in embedding plots, the new dataset exhibits a significantly wider and denser range of hand poses compared to existing benchmarks. Current state-of-the-art methods are evaluated on the dataset, and we demonstrate significant improvements in cross-benchmark performance. We also show significant improvements in egocentric hand pose estimation with a CNN trained on the new dataset.},
 author = {Yuan, Shanxin and Ye, Qi and Stenger, Bjorn and Jain, Siddhant and Kim, Tae-Kyun},
 date = {4/9/2017},
 title = {{BigHand2.2M Benchmark: Hand Pose Dataset and State of the Art Analysis}},
 url = {http://arxiv.org/pdf/1704.02612v2},
 keywords = {Computer Science - Computer Vision and Pattern Recognition}
}


@misc{Yuan2017b,
 abstract = {In this paper, we strive to answer two questions: What is the current state of 3D hand pose estimation from depth images? And, what are the next challenges that need to be tackled? Following the successful Hands In the Million Challenge (HIM2017), we investigate the top 10 state-of-the-art methods on three tasks: single frame 3D pose estimation, 3D hand tracking, and hand pose estimation during object interaction. We analyze the performance of different CNN structures with regard to hand shape, joint visibility, view point and articulation distributions. Our findings include: (1) isolated 3D hand pose estimation achieves low mean errors (10 mm) in the view point range of [70, 120] degrees, but it is far from being solved for extreme view points; (2) 3D volumetric representations outperform 2D CNNs, better capturing the spatial structure of the depth data; (3) Discriminative methods still generalize poorly to unseen hand shapes; (4) While joint occlusions pose a challenge for most methods, explicit modeling of structure constraints can significantly narrow the gap between errors on visible and occluded joints.},
 author = {Yuan, Shanxin and Garcia-Hernando, Guillermo and Stenger, Bjorn and Moon, Gyeongsik and Chang, Ju Yong and Lee, Kyoung Mu and Molchanov, Pavlo and Kautz, Jan and Honari, Sina and Ge, Liuhao and Yuan, Junsong and Chen, Xinghao and Wang, Guijin and Yang, Fan and Akiyama, Kai and Wu, Yang and Wan, Qingfu and Madadi, Meysam and Escalera, Sergio and Li, Shile and Lee, Dongheui and Oikonomidis, Iason and Argyros, Antonis and Kim, Tae-Kyun},
 date = {12/11/2017},
 title = {{Depth-Based 3D Hand Pose Estimation: From Current Achievements to Future  Goals}},
 url = {http://arxiv.org/pdf/1712.03917v2},
 keywords = {Computer Science - Computer Vision and Pattern Recognition}
}


@misc{Yuan2017c,
 abstract = {We present the 2017 Hands in the Million Challenge, a public competition designed for the evaluation of the task of 3D hand pose estimation. The goal of this challenge is to assess how far is the state of the art in terms of solving the problem of 3D hand pose estimation as well as detect major failure and strength modes of both systems and evaluation metrics that can help to identify future research directions. The challenge follows up the recent publication of BigHand2.2M and First-Person Hand Action datasets, which have been designed to exhaustively cover multiple hand, viewpoint, hand articulation, and occlusion. The challenge consists of a standardized dataset, an evaluation protocol for two different tasks, and a public competition. In this document we describe the different aspects of the challenge and, jointly with the results of the participants, it will be presented at the 3rd International Workshop on Observing and Understanding Hands in Action, HANDS 2017, with ICCV 2017.},
 author = {Yuan, Shanxin and Ye, Qi and Garcia-Hernando, Guillermo and Kim, Tae-Kyun},
 date = {7/7/2017},
 title = {{The 2017 Hands in the Million Challenge on 3D Hand Pose Estimation}},
 url = {http://arxiv.org/pdf/1707.02237v1},
 keywords = {BigHands2.2m;Computer Science - Computer Vision and Pattern Recognition}
}


@misc{Yuan2018,
 abstract = {This paper proposes a method for hand pose estimation from RGB images that uses both external large-scale depth image datasets and paired depth and RGB images as privileged information at training time. We show that providing depth information during training significantly improves performance of pose estimation from RGB images during testing. We explore different ways of using this privileged information: (1) using depth data to initially train a depth-based network, (2) using the features from the depth-based network of the paired depth images to constrain mid-level RGB network weights, and (3) using the foreground mask, obtained from the depth data, to suppress the responses from the background area. By using paired RGB and depth images, we are able to supervise the RGB-based network to learn middle layer features that mimic that of the corresponding depth-based network, which is trained on large-scale, accurately annotated depth data. During testing, when only an RGB image is available, our method produces accurate 3D hand pose predictions. Our method is also tested on 2D hand pose estimation. Experiments on three public datasets show that the method outperforms the state-of-the-art methods for hand pose estimation using RGB image input.},
 author = {Yuan, Shanxin and Stenger, Bjorn and Kim, Tae-Kyun},
 date = {18.11.2018},
 title = {{RGB-based 3D Hand Pose Estimation via Privileged Learning with Depth  Images}},
 url = {http://arxiv.org/pdf/1811.07376v1}
}


@misc{Zeiler2013,
 abstract = {Large Convolutional Network models have recently demonstrated impressive classification performance on the ImageNet benchmark. However there is no clear understanding of why they perform so well, or how they might be improved. In this paper we address both issues. We introduce a novel visualization technique that gives insight into the function of intermediate feature layers and the operation of the classifier. We also perform an ablation study to discover the performance contribution from different model layers. This enables us to find model architectures that outperform Krizhevsky $\backslash$etal on the ImageNet classification benchmark. We show our ImageNet model generalizes well to other datasets: when the softmax classifier is retrained, it convincingly beats the current state-of-the-art results on Caltech-101 and Caltech-256 datasets.},
 author = {Zeiler, Matthew D. and Fergus, Rob},
 date = {12.11.2013},
 title = {{Visualizing and Understanding Convolutional Networks}},
 url = {http://arxiv.org/pdf/1311.2901v3}
}


@misc{Zhang2016,
 abstract = {3D hand pose tracking/estimation will be very important in the next generation of human-computer interaction. Most of the currently available algorithms rely on low-cost active depth sensors. However, these sensors can be easily interfered by other active sources and require relatively high power consumption. As a result, they are currently not suitable for outdoor environments and mobile devices. This paper aims at tracking/estimating hand poses using passive stereo which avoids these limitations. A benchmark with 18,000 stereo image pairs and 18,000 depth images captured from different scenarios and the ground-truth 3D positions of palm and finger joints (obtained from the manual label) is thus proposed. This paper demonstrates that the performance of the state-of-the art tracking/estimation algorithms can be maintained with most stereo matching algorithms on the proposed benchmark, as long as the hand segmentation is correct. As a result, a novel stereo-based hand segmentation algorithm specially designed for hand tracking/estimation is proposed. The quantitative evaluation demonstrates that the proposed algorithm is suitable for the state-of-the-art hand pose tracking/estimation algorithms and the tracking quality is comparable to the use of active depth sensors under different challenging scenarios.},
 author = {Zhang, Jiawei and Jiao, Jianbo and Chen, Mingliang and Qu, Liangqiong and Xu, Xiaobin and Yang, Qingxiong},
 date = {23.10.2016},
 title = {{3D Hand Pose Tracking and Estimation Using Stereo Matching}},
 url = {http://arxiv.org/pdf/1610.07214v1},
 keywords = {Computer Science - Computer Vision and Pattern Recognition}
}


@misc{Zimmermann2017,
 abstract = {Low-cost consumer depth cameras and deep learning have enabled reasonable 3D hand pose estimation from single depth images. In this paper, we present an approach that estimates 3D hand pose from regular RGB images. This task has far more ambiguities due to the missing depth information. To this end, we propose a deep network that learns a network-implicit 3D articulation prior. Together with detected keypoints in the images, this network yields good estimates of the 3D pose. We introduce a large scale 3D hand pose dataset based on synthetic hand models for training the involved networks. Experiments on a variety of test sets, including one on sign language recognition, demonstrate the feasibility of 3D hand pose estimation on single color images.},
 author = {Zimmermann, Christian and Brox, Thomas},
 date = {03.05.2017},
 title = {{Learning to Estimate 3D Hand Pose from Single RGB Images}},
 url = {http://arxiv.org/pdf/1705.01389v3},
 keywords = {Algorithms;Algorithmus;Artificial Intelligence (incl. Robotics);Bayes-Verfahren;Bildverarbeitung;Computer Imaging, Vision, Pattern Recognition and Graphics;Computer Science - Computer Vision and Pattern Recognition;Dataset;Gestures;Humans;Image Processing and Computer Vision;Learning;Lehrbuch;Maschinelles Sehen;Neural Networks (Computer);Normal Distribution;Pattern Recognition;Pattern Recognition, Automated}
}


