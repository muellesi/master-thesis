% This file was created with Citavi 6.3.0.0

@misc{Malik8282018,
 abstract = {Articulated hand pose and shape estimation is an important problem for vision-based applications such as augmented reality and animation. In contrast to the existing methods which optimize only for joint positions, we propose a fully supervised deep network which learns to jointly estimate a full 3D hand mesh representation and pose from a single depth image. To this end, a CNN architecture is employed to estimate parametric representations i.e. hand pose, bone scales and complex shape parameters. Then, a novel hand pose and shape layer, embedded inside our deep framework, produces 3D joint positions and hand mesh. Lack of sufficient training data with varying hand shapes limits the generalized performance of learning based methods. Also, manually annotating real data is suboptimal. Therefore, we present SynHand5M: a million-scale synthetic dataset with accurate joint annotations, segmentation masks and mesh files of depth maps. Among model based learning (hybrid) methods, we show improved results on our dataset and two of the public benchmarks i.e. NYU and ICVL. Also, by employing a joint training strategy with real and synthetic data, we recover 3D hand mesh and pose from real images in 3.7ms.},
 author = {Malik, Jameel and Elhayek, Ahmed and Nunnari, Fabrizio and Varanasi, Kiran and Tamaddon, Kiarash and Heloir, Alexis and Stricker, Didier},
 date = {8/28/2018},
 title = {{DeepHPS: End-to-end Estimation of 3D Hand Pose and Shape by Learning  from Synthetic Depth}},
 url = {http://arxiv.org/pdf/1808.09208v1},
 keywords = {Computer Science - Computer Vision and Pattern Recognition}
}


@article{Li2019,
 author = {Li, Yong and He, Zihang and Ye, Xiang and He, Zuguo and Han, Kangrong},
 year = {2019},
 title = {{Spatial temporal graph convolutional networks for skeleton-based dynamic hand gesture recognition}},
 pages = {404},
 volume = {2019},
 number = {1},
 journal = {{EURASIP Journal on Image and Video Processing}},
 doi = {10.1186/s13640-019-0476-x}
}


@proceedings{Liu2017,
 year = {2017},
 title = {{MM'17: Proceedings of the 2017 ACM Multimedia Conference : October 23-27, 2017, Amsterdam, the Netherlands}},
 address = {New York, NY, USA},
 publisher = {{ACM Association for Computing Machinery}},
 isbn = {9781450349062},
 editor = {Liu, Qiong and Lienhart, Rainer and Wang, Haohong and Chen, Sheng-Wei {\textquotedbl}Kuan-Ta{\textquotedbl} and Boll, Susanne and Chen, Phoebe and Friedland, Gerald and Li, Jia and Yan, Shuicheng},
 institution = {{ACM Multimedia Conference} and {Association for Computing Machinery} and {ACM Multimedia} and MM},
 doi = {10.1145/3123266}
}


@misc{Kopuklu1292019,
 abstract = {Real-time recognition of dynamic hand gestures from video streams is a challenging task since (i) there is no indication when a gesture starts and ends in the video, (ii) performed gestures should only be recognized once, and (iii) the entire architecture should be designed considering the memory and power budget. In this work, we address these challenges by proposing a hierarchical structure enabling offline-working convolutional neural network (CNN) architectures to operate online efficiently by using sliding window approach. The proposed architecture consists of two models: (1) A detector which is a lightweight CNN architecture to detect gestures and (2) a classifier which is a deep CNN to classify the detected gestures. In order to evaluate the single-time activations of the detected gestures, we propose to use Levenshtein distance as an evaluation metric since it can measure misclassifications, multiple detections, and missing detections at the same time. We evaluate our architecture on two publicly available datasets - EgoGesture and NVIDIA Dynamic Hand Gesture Datasets - which require temporal detection and classification of the performed hand gestures. ResNeXt-101 model, which is used as a classifier, achieves the state-of-the-art offline classification accuracy of 94.04{\%} and 83.82{\%} for depth modality on EgoGesture and NVIDIA benchmarks, respectively. In real-time detection and classification, we obtain considerable early detections while achieving performances close to offline operation. The codes and pretrained models used in this work are publicly available.},
 author = {K{\"o}p{\"u}kl{\"u}, Okan and Gunduz, Ahmet and Kose, Neslihan and Rigoll, Gerhard},
 date = {1/29/2019},
 title = {{Real-time Hand Gesture Detection and Classification Using Convolutional  Neural Networks}},
 url = {http://arxiv.org/pdf/1901.10323v2},
 keywords = {Computer Science - Artificial Intelligence;Computer Science - Computer Vision and Pattern Recognition}
}


@article{Liu2016,
 author = {Liu, Zhi and Zhang, Chenyang and Tian, Yingli},
 year = {2016},
 title = {{3D-based Deep Convolutional Neural Network for action recognition with depth sequences}},
 pages = {93--100},
 volume = {55},
 issn = {02628856},
 journal = {{Image and Vision Computing}},
 doi = {10.1016/j.imavis.2016.04.004}
}


@proceedings{Lu2011,
 abstract = {Computer vision system is one of the newest approaches for human computer interaction. Recently, the direct use of our hands as natural input devices has shown promising progress. Toward this progress, we introduce a hand gesture recognition system in this study to recognize real time gesture in unconstrained environments. The system consists of three components: real time hand tracking, hand-tree construction, and hand gesture recognition. Our main contribution includes: (1) a simple way to represent the hand gesture after applying thinning algorithm to the image, and (2) using a model of complex-valued neural network (CVNN) for real-valued classification. We have tested our system to 26 different gestures to evaluate the effectiveness of our approach. The results show that the classification ability of single-layered CVNN on unseen data is comparable to the conventional real-valued neural network (RVNN) having one hidden layer. Moreover, convergence of the CVNN is much faster than that of the RVNN in most cases.},
 year = {2011},
 title = {{Real-Time Hand Gesture Recognition Using Complex-Valued Neural Network (CVNN): Neural Information Processing}},
 publisher = {{Springer Berlin Heidelberg}},
 isbn = {978-3-642-24955-6},
 editor = {Lu, Bao-Liang and Zhang, Liqing and Kwok, James and Hafiz, Abdul Rahman and Amin, Md. Faijul and Murase, Kazuyuki}
}


@misc{Maddison12202014,
 abstract = {The game of Go is more challenging than other board games, due to the difficulty of constructing a position or move evaluation function. In this paper we investigate whether deep convolutional networks can be used to directly represent and learn this knowledge. We train a large 12-layer convolutional neural network by supervised learning from a database of human professional games. The network correctly predicts the expert move in 55{\%} of positions, equalling the accuracy of a 6 dan human player. When the trained convolutional network was used directly to play games of Go, without any search, it beat the traditional search program GnuGo in 97{\%} of games, and matched the performance of a state-of-the-art Monte-Carlo tree search that simulates a million positions per move.},
 author = {Maddison, Chris J. and Huang, Aja and Sutskever, Ilya and Silver, David},
 date = {12/20/2014},
 title = {{Move Evaluation in Go Using Deep Convolutional Neural Networks}},
 url = {http://arxiv.org/pdf/1412.6564v2},
 keywords = {Computer Science - Learning;Computer Science - Neural and Evolutionary Computing}
}


@proceedings{InstituteofElectricalandElectronicsEngineers2015,
 year = {2015},
 title = {{2015 IEEE Conference on Computer Vision and Pattern Recognition (CVPR): - 12 June 2015, Boston, MA}},
 keywords = {Computer vision;Congresses;Image processing;Maschinelles Sehen;Mustererkennung;Optical pattern recognition;Pattern recognition systems},
 address = {Piscataway, NJ},
 publisher = {IEEE},
 isbn = {978-1-4673-6964-0},
 institution = {{Institute of Electrical and Electronics Engineers} and {IEEE Conference on Computer Vision and Pattern Recognition} and CVPR}
}


@misc{Malik1282017,
 abstract = {Articulated hand pose estimation is a challenging task for human-computer interaction. The state-of-the-art hand pose estimation algorithms work only with one or a few subjects for which they have been calibrated or trained. Particularly, the hybrid methods based on learning followed by model fitting or model based deep learning do not explicitly consider varying hand shapes and sizes. In this work, we introduce a novel hybrid algorithm for estimating the 3D hand pose as well as bone-lengths of the hand skeleton at the same time, from a single depth image. The proposed CNN architecture learns hand pose parameters and scale parameters associated with the bone-lengths simultaneously. Subsequently, a new hybrid forward kinematics layer employs both parameters to estimate 3D joint positions of the hand. For end-to-end training, we combine three public datasets NYU, ICVL and MSRA-2015 in one unified format to achieve large variation in hand shapes and sizes. Among hybrid methods, our method shows improved accuracy over the state-of-the-art on the combined dataset and the ICVL dataset that contain multiple subjects. Also, our algorithm is demonstrated to work well with unseen images.},
 author = {Malik, Jameel and Elhayek, Ahmed and Stricker, Didier},
 date = {12/8/2017},
 title = {{Simultaneous Hand Pose and Skeleton Bone-Lengths Estimation from a  Single Depth Image}},
 url = {http://arxiv.org/pdf/1712.03121v1},
 keywords = {Computer Science - Computer Vision and Pattern Recognition;Computer Science - Human-Computer Interaction}
}


@proceedings{IEEEInternationalConferenceonAutomaticFaceandGestureRecognition2018,
 year = {2018},
 title = {{13th IEEE International Conference on Automatic Face and Gesture Recognition: FG 2018 : 15-19 May 2018, Xi'an, China : proceedings}},
 keywords = {Gesicht;Mustererkennung},
 address = {Piscataway, NJ},
 publisher = {IEEE},
 isbn = {978-1-5386-2335-0},
 institution = {{IEEE International Conference on Automatic Face and Gesture Recognition} and {Institute of Electrical and Electronics Engineers} and FG}
}


@incollection{Malik2018,
 crossref = {Bourdot2018},
 author = {Malik, Jameel and Elhayek, Ahmed and Stricker, Didier},
 title = {{Structure-Aware 3D Hand Pose Regression from a Single Depth Image}},
 pages = {3--17},
 volume = {11162},
 publisher = {Springer},
 isbn = {978-3-030-01789-7},
 series = {{Lecture Notes in Computer Science}},
 editor = {Bourdot, Patrick and Cobb, Sue and Interrante, Victoria and kato, Hirokazu and Stricker, Didier},
 booktitle = {{Virtual reality and augmented reality}},
 year = {2018},
 address = {Cham},
 doi = {10.1007/978-3-030-01790-3{\textunderscore }1}
}


@misc{Melax5222017,
 abstract = {Tracking the full skeletal pose of the hands and fingers is a challenging problem that has a plethora of applications for user interaction. Existing techniques either require wearable hardware, add restrictions to user pose, or require significant computation resources. This research explores a new approach to tracking hands, or any articulated model, by using an augmented rigid body simulation. This allows us to phrase 3D object tracking as a linear complementarity problem with a well-defined solution. Based on a depth sensor's samples, the system generates constraints that limit motion orthogonal to the rigid body model's surface. These constraints, along with prior motion, collision/contact constraints, and joint mechanics, are resolved with a projected Gauss-Seidel solver. Due to camera noise properties and attachment errors, the numerous surface constraints are impulse capped to avoid overpowering mechanical constraints. To improve tracking accuracy, multiple simulations are spawned at each frame and fed a variety of heuristics, constraints and poses. A 3D error metric selects the best-fit simulation, helping the system handle challenging hand motions. Such an approach enables real-time, robust, and accurate 3D skeletal tracking of a user's hand on a variety of depth cameras, while only utilizing a single x86 CPU core for processing.},
 author = {Melax, Stan and Keselman, Leonid and Orsten, Sterling},
 date = {5/22/2017},
 title = {{Dynamics Based 3D Skeletal Hand Tracking}},
 url = {http://arxiv.org/pdf/1705.07640v1},
 keywords = {Computer Science - Computer Vision and Pattern Recognition;Computer Science - Graphics}
}


@inproceedings{Hoang2018,
 crossref = {Unknown2018},
 author = {Hoang, Nguyen Ngoc and Lee, Guee-Sang and Kim, Soo-Hyung and Yang, Hyung-Jeong},
 title = {{A Real-time Multimodal Hand Gesture Recognition via 3D Convolutional Neural Network and Key Frame Extraction}},
 pages = {32--37},
 publisher = {{ACM Press}},
 isbn = {9781450365567},
 editor = {Unknown},
 booktitle = {{Proceedings of the 2018 International Conference on Machine Learning and Machine Intelligence  - MLMI2018}},
 year = {2018},
 address = {New York, New York, USA},
 doi = {10.1145/3278312.3278314}
}


@inproceedings{HernandezRuiz2017,
 crossref = {Liu2017},
 author = {{Hernandez Ruiz}, Alejandro and Porzi, Lorenzo and {Rota Bul{\`o}}, Samuel and Moreno-Noguer, Francesc},
 title = {{3D CNNs on Distance Matrices for Human Action Recognition}},
 pages = {1087--1095},
 publisher = {{ACM Association for Computing Machinery}},
 isbn = {9781450349062},
 editor = {Liu, Qiong and Lienhart, Rainer and Wang, Haohong and Chen, Sheng-Wei {\textquotedbl}Kuan-Ta{\textquotedbl} and Boll, Susanne and Chen, Phoebe and Friedland, Gerald and Li, Jia and Yan, Shuicheng},
 booktitle = {{MM'17}},
 year = {2017},
 address = {New York, NY, USA},
 doi = {10.1145/3123266.3123299}
}


@misc{Hampali722019,
 abstract = {We propose a new dataset for 3D hand+object pose estimation from color images, together with a method for efficiently annotating this dataset, and a 3D pose prediction method based on this dataset. The current lack of training data makes the 3D hand+object pose estimation very challenging. This lack is due to the complexity of labeling many real images with both 3D poses and of generating synthetic images with various realistic interaction. Moreover, even if synthetic images could be used for training, annotated real images are still needed for validation. To tackle this challenge, we capture sequences with a simple setup made of a single RGB-D camera. We also use a color camera imaging the sequences from a side view, but only for validation. We introduce a novel method based on global optimization that exploits depth, color, and temporal constraints for efficiently annotating the sequences, which we use to train another novel method that predicts both the 3D poses of the hand and the object from a single color image. Our hope is to encourage other researchers to develop better annotation methods for our dataset: One can then apply such method to capture and easily annotate sequences captured with a single RGB-D camera to easily create additional training data thus solving one of the main problems of 3D hand+object pose estimation.},
 author = {Hampali, Shreyas and Oberweger, Markus and Rad, Mahdi and Lepetit, Vincent},
 date = {7/2/2019},
 title = {{HO-3D: A Multi-User, Multi-Object Dataset for Joint 3D Hand-Object Pose  Estimation}},
 url = {http://arxiv.org/pdf/1907.01481v1},
 keywords = {Computer Science - Computer Vision and Pattern Recognition}
}


@article{Li2018,
 abstract = {Information Sciences, 441 (2018) 66-78. doi:10.1016/j.ins.2018.02.024},
 author = {Li, Yuan and Wang, Xinggang and Liu, Wenyu and Feng, Bin},
 year = {2018},
 title = {{Deep attention network for joint hand gesture localization and recognition using static RGB-D images}},
 keywords = {CNN;Hand gesture localization and recognition;RGB-D images;Soft attention mechanism},
 pages = {66--78},
 volume = {441},
 issn = {00200255},
 journal = {{Information Sciences}},
 doi = {10.1016/j.ins.2018.02.024}
}


@article{Hafiz2015,
 abstract = {Complex-valued neural networks (CVNNs), that allow processing complex-valued data directly, have been applied to a number of practical applications, especially in signal and image processing. In this paper, we apply CVNN as a classification algorithm for the skeletal wireframe data that are generated from hand gestures. A CVNN having one hidden layer that maps complex-valued input to real-valued output was used, a training algorithm based on Levenberg Marquardt algorithm (CLMA) was derived, and a task to recognize 26 different gestures that represent English alphabet was given. The initial image processing part consists of three modules: real-time hand tracking, hand-skeletal construction, and hand gesture recognition. We have achieved; (1) efficient and accurate gesture extraction and representation in complex domain, (2) training of the CVNN utilising CLMA, and (3) providing a proof of the superiority of the aforementioned methods by utilising complex-valued learning vector quantization. A comparison with real-valued neural network shows that a CVNN with CLMA provides higher recognition performance, accompanied by significantly faster training. Moreover, a comparison of six different activation functions was performed and their utility is argued.},
 author = {Hafiz, Abdul Rahman and Al-Nuaimi, Ahmed Yarub and Amin, Md. Faijul and Murase, Kazuyuki},
 year = {2015},
 title = {{Classification of Skeletal Wireframe Representation of Hand Gesture Using Complex-Valued Neural Network}},
 url = {https://doi.org/10.1007/s11063-014-9379-0},
 pages = {649--664},
 volume = {42},
 number = {3},
 issn = {1573-773X},
 journal = {{Neural Processing Letters}},
 doi = {10.1007/s11063-014-9379-0}
}


@misc{Ge6232016,
 abstract = {Articulated hand pose estimation plays an important role in human-computer interaction. Despite the recent progress, the accuracy of existing methods is still not satisfactory, partially due to the difficulty of embedded high-dimensional and non-linear regression problem. Different from the existing discriminative methods that regress for the hand pose with a single depth image, we propose to first project the query depth image onto three orthogonal planes and utilize these multi-view projections to regress for 2D heat-maps which estimate the joint positions on each plane. These multi-view heat-maps are then fused to produce final 3D hand pose estimation with learned pose priors. Experiments show that the proposed method largely outperforms state-of-the-art on a challenging dataset. Moreover, a cross-dataset experiment also demonstrates the good generalization ability of the proposed method.},
 author = {Ge, Liuhao and Liang, Hui and Yuan, Junsong and Thalmann, Daniel},
 date = {6/23/2016},
 title = {{Robust 3D Hand Pose Estimation in Single Depth Images: from Single-View  CNN to Multi-View CNNs}},
 url = {http://arxiv.org/pdf/1606.07253v3},
 keywords = {Computer Science - Computer Vision and Pattern Recognition}
}


@article{Patrona2018,
 abstract = {Pattern Recognition, 76 (2017) 612-622. doi:10.1016/j.patcog.2017.12.007},
 author = {Patrona, Fotini and Chatzitofis, Anargyros and Zarpalas, Dimitrios and Daras, Petros},
 year = {2018},
 title = {{Motion analysis: Action detection, recognition and evaluation based on motion capture data}},
 keywords = {Automatic joint/angle weighting;Kinect;Kinetic energy;Motion evaluation;Online human action detection;Online human action recognition;Skeleton data},
 pages = {612--622},
 volume = {76},
 issn = {00313203},
 journal = {{Pattern Recognition}},
 doi = {10.1016/j.patcog.2017.12.007}
}


@misc{Shotton2011,
 author = {Shotton, Jamie and Fitzgibbon, Andrew and Blake, Andrew and Kipman, Alex and Finocchio, Mark and Moore, Bob and Sharp, Toby},
 year = {2011},
 title = {{Real-Time Human Pose Recognition in Parts from a Single Depth Image}},
 url = {https://www.microsoft.com/en-us/research/wp-content/uploads/2016/02/BodyPartRecognition.pdf}
}


@article{Farooq2015,
 author = {Farooq, Adnan and Won, Chee Sun},
 year = {2015},
 title = {{A Survey of Human Action Recognition Approaches that use an RGB-D Sensor}},
 pages = {281--290},
 volume = {4},
 number = {4},
 journal = {{IEIE Transactions on Smart Processing and Computing}},
 doi = {10.5573/IEIESPC.2015.4.4.281}
}


@misc{Simon25.04.2017,
 abstract = {We present an approach that uses a multi-camera system to train fine-grained detectors for keypoints that are prone to occlusion, such as the joints of a hand. We call this procedure multiview bootstrapping: first, an initial keypoint detector is used to produce noisy labels in multiple views of the hand. The noisy detections are then triangulated in 3D using multiview geometry or marked as outliers. Finally, the reprojected triangulations are used as new labeled training data to improve the detector. We repeat this process, generating more labeled data in each iteration. We derive a result analytically relating the minimum number of views to achieve target true and false positive rates for a given detector. The method is used to train a hand keypoint detector for single images. The resulting keypoint detector runs in realtime on RGB images and has accuracy comparable to methods that use depth sensors. The single view detector, triangulated over multiple views, enables 3D markerless hand motion capture with complex object interactions.},
 author = {Simon, Tomas and Joo, Hanbyul and Matthews, Iain and Sheikh, Yaser},
 date = {25.04.2017},
 title = {{Hand Keypoint Detection in Single Images using Multiview Bootstrapping}},
 url = {http://arxiv.org/pdf/1704.07809v1}
}


@inproceedings{Sun2015,
 crossref = {InstituteofElectricalandElectronicsEngineers2015},
 author = {Sun, Xiao and Wei, Yichen and Liang, Shuang and Tang, Xiaoou and Sun, Jian},
 title = {{Cascaded hand pose regression}},
 pages = {824--832},
 publisher = {IEEE},
 isbn = {978-1-4673-6964-0},
 booktitle = {{2015 IEEE Conference on Computer Vision and Pattern Recognition (CVPR)}},
 year = {2015},
 address = {Piscataway, NJ},
 doi = {10.1109/CVPR.2015.7298683}
}


@proceedings{Unknown2018,
 year = {2018},
 title = {{Proceedings of the 2018 International Conference on Machine Learning and Machine Intelligence  - MLMI2018}},
 address = {New York, New York, USA},
 publisher = {{ACM Press}},
 isbn = {9781450365567},
 editor = {Unknown},
 doi = {10.1145/3278312}
}


@proceedings{WorkshoponHumanMotion2000,
 year = {2000},
 title = {{Proceedings, Workshop on Human Motion: 7-8 December 2000, Austin, Texas}},
 keywords = {Automation;Computer simulation;Computer vision;Congresses;Data processing;Face perception;Gesture;Human locomotion;Kinesiology;Optical pattern recognition},
 address = {Los Alamitos, Calif},
 publisher = {{IEEE Computer Society}},
 isbn = {0-7695-0939-8},
 institution = {{Workshop on Human Motion} and {IEEE Computer Society}}
}


@book{Bourdot2018,
 year = {2018},
 title = {{Virtual reality and augmented reality: 15th EuroVR International Conference, EuroVR 2018, London, UK, October 22--23, 2018 : proceedings}},
 price = {(Festeinband : EUR 62.05 (DE) (freier Preis), EUR 63.79 (AT) (freier Preis), CHF 64.00 (freier Preis))},
 keywords = {Erweiterte Realit{\"a}t;Erweiterte Realit{\"a}t {\textless}Informatik{\textgreater};Virtuelle Realit{\"a}t;Virtuelle Welten},
 address = {Cham},
 volume = {11162},
 publisher = {Springer},
 isbn = {978-3-030-01789-7},
 series = {{Lecture Notes in Computer Science}},
 editor = {Bourdot, Patrick and Cobb, Sue and Interrante, Victoria and kato, Hirokazu and Stricker, Didier},
 doi = {10.1007/978-3-030-01790-3}
}


@article{Wu2016,
 author = {Wu, Chih-Hung and Chen, Wei-Lun and Lin, Chang Hong},
 year = {2016},
 title = {{Depth-based hand gesture recognition}},
 pages = {7065--7086},
 volume = {75},
 number = {12},
 issn = {1380-7501},
 journal = {{Multimedia Tools and Applications}},
 doi = {10.1007/s11042-015-2632-3}
}


@article{Xin2016,
 abstract = {Recognition of human actions from digital video is a challenging task due to complex interfering factors in uncontrolled realistic environments. In this paper, we propose a learning framework using static, dynamic and sequential mixed features to solve three fundamental problems: spatial domain variation, temporal domain polytrope, and intra- and inter-class diversities. Utilizing a cognitive-based data reduction method and a hybrid {\textquotedbl}network upon networks{\textquotedbl} architecture, we extract human action representations which are robust against spatial and temporal interferences and adaptive to variations in both action speed and duration. We evaluated our method on the UCF101 and other three challenging datasets. Our results demonstrated a superior performance of the proposed algorithm in human action recognition.},
 author = {Xin, Miao and Zhang, Hong and Wang, Helong and Sun, Mingui and Yuan, Ding},
 year = {2016},
 title = {{ARCH: Adaptive recurrent-convolutional hybrid networks for long-term action recognition}},
 keywords = {Action recognition;Deep learning;Hybrid feature learning},
 pages = {87--102},
 volume = {178},
 issn = {0925-2312},
 journal = {{Neurocomputing}},
 doi = {10.1016/j.neucom.2015.09.112}
}


@misc{Yuan12112017,
 abstract = {In this paper, we strive to answer two questions: What is the current state of 3D hand pose estimation from depth images? And, what are the next challenges that need to be tackled? Following the successful Hands In the Million Challenge (HIM2017), we investigate the top 10 state-of-the-art methods on three tasks: single frame 3D pose estimation, 3D hand tracking, and hand pose estimation during object interaction. We analyze the performance of different CNN structures with regard to hand shape, joint visibility, view point and articulation distributions. Our findings include: (1) isolated 3D hand pose estimation achieves low mean errors (10 mm) in the view point range of [70, 120] degrees, but it is far from being solved for extreme view points; (2) 3D volumetric representations outperform 2D CNNs, better capturing the spatial structure of the depth data; (3) Discriminative methods still generalize poorly to unseen hand shapes; (4) While joint occlusions pose a challenge for most methods, explicit modeling of structure constraints can significantly narrow the gap between errors on visible and occluded joints.},
 author = {Yuan, Shanxin and Garcia-Hernando, Guillermo and Stenger, Bjorn and Moon, Gyeongsik and Chang, Ju Yong and Lee, Kyoung Mu and Molchanov, Pavlo and Kautz, Jan and Honari, Sina and Ge, Liuhao and Yuan, Junsong and Chen, Xinghao and Wang, Guijin and Yang, Fan and Akiyama, Kai and Wu, Yang and Wan, Qingfu and Madadi, Meysam and Escalera, Sergio and Li, Shile and Lee, Dongheui and Oikonomidis, Iason and Argyros, Antonis and Kim, Tae-Kyun},
 date = {12/11/2017},
 title = {{Depth-Based 3D Hand Pose Estimation: From Current Achievements to Future  Goals}},
 url = {http://arxiv.org/pdf/1712.03917v2},
 keywords = {Computer Science - Computer Vision and Pattern Recognition}
}


@misc{Yuan772017,
 abstract = {We present the 2017 Hands in the Million Challenge, a public competition designed for the evaluation of the task of 3D hand pose estimation. The goal of this challenge is to assess how far is the state of the art in terms of solving the problem of 3D hand pose estimation as well as detect major failure and strength modes of both systems and evaluation metrics that can help to identify future research directions. The challenge follows up the recent publication of BigHand2.2M and First-Person Hand Action datasets, which have been designed to exhaustively cover multiple hand, viewpoint, hand articulation, and occlusion. The challenge consists of a standardized dataset, an evaluation protocol for two different tasks, and a public competition. In this document we describe the different aspects of the challenge and, jointly with the results of the participants, it will be presented at the 3rd International Workshop on Observing and Understanding Hands in Action, HANDS 2017, with ICCV 2017.},
 author = {Yuan, Shanxin and Ye, Qi and Garcia-Hernando, Guillermo and Kim, Tae-Kyun},
 date = {7/7/2017},
 title = {{The 2017 Hands in the Million Challenge on 3D Hand Pose Estimation}},
 url = {http://arxiv.org/pdf/1707.02237v1},
 keywords = {Computer Science - Computer Vision and Pattern Recognition}
}


@misc{Yuan492017,
 abstract = {In this paper we introduce a large-scale hand pose dataset, collected using a novel capture method. Existing datasets are either generated synthetically or captured using depth sensors: synthetic datasets exhibit a certain level of appearance difference from real depth images, and real datasets are limited in quantity and coverage, mainly due to the difficulty to annotate them. We propose a tracking system with six 6D magnetic sensors and inverse kinematics to automatically obtain 21-joints hand pose annotations of depth maps captured with minimal restriction on the range of motion. The capture protocol aims to fully cover the natural hand pose space. As shown in embedding plots, the new dataset exhibits a significantly wider and denser range of hand poses compared to existing benchmarks. Current state-of-the-art methods are evaluated on the dataset, and we demonstrate significant improvements in cross-benchmark performance. We also show significant improvements in egocentric hand pose estimation with a CNN trained on the new dataset.},
 author = {Yuan, Shanxin and Ye, Qi and Stenger, Bjorn and Jain, Siddhant and Kim, Tae-Kyun},
 date = {4/9/2017},
 title = {{BigHand2.2M Benchmark: Hand Pose Dataset and State of the Art Analysis}},
 url = {http://arxiv.org/pdf/1704.02612v2},
 keywords = {Computer Science - Computer Vision and Pattern Recognition}
}


@article{Nunez2018,
 abstract = {Pattern Recognition, 76 (2017) 80-94. doi:10.1016/j.patcog.2017.10.033},
 author = {N{\'u}{\~n}ez, Juan C. and Cabido, Ra{\'u}l and Pantrigo, Juan J. and Montemayor, Antonio S. and V{\'e}lez, Jos{\'e} F.},
 year = {2018},
 title = {{Convolutional Neural Networks and Long Short-Term Memory for skeleton-based human activity and hand gesture recognition}},
 keywords = {Convolutional Neural Network;Deep learning;Hand gesture recognition;Human activity recognition;Long Short-Term Memory;Real-time;Recurrent neural network},
 pages = {80--94},
 volume = {76},
 issn = {00313203},
 journal = {{Pattern Recognition}},
 doi = {10.1016/j.patcog.2017.10.033}
}


@inproceedings{Lin2000,
 crossref = {WorkshoponHumanMotion2000},
 author = {Lin, John and Wu, Ying and Huang, T. S.},
 title = {{Modeling the constraints of human hand motion}},
 pages = {121--126},
 publisher = {{IEEE Computer Society}},
 isbn = {0-7695-0939-8},
 booktitle = {{Proceedings, Workshop on Human Motion}},
 year = {2000},
 address = {Los Alamitos, Calif},
 doi = {10.1109/HUMO.2000.897381}
}


@proceedings{1317March2001,
 year = {13-17 March 2001},
 title = {{Proceedings IEEE Virtual Reality 2001}},
 publisher = {{IEEE Comput. Soc}},
 isbn = {0-7695-0948-7}
}


@proceedings{1317March2001b,
 year = {13-17 March 2001},
 title = {{Proceedings IEEE Virtual Reality 2001}},
 publisher = {{IEEE Comput. Soc}},
 isbn = {0-7695-0948-7}
}


@book{2006,
 year = {2006},
 title = {{Proceedings of the 5th international conference on devlopment and learning ICDL 2006: Indiana University, Bloomington, IN : May 31 - June 3rd, 2006}},
 address = {[Bloomington, Ind.]},
 publisher = {{Department of Psychological and Brain Sciences, Indiana University}},
 isbn = {9780978645601}
}


@proceedings{2013,
 year = {2013},
 title = {{Proceedings of the IEEE International Conference on Computer Vision (ICCV)}}
}


@proceedings{2014,
 year = {2014},
 title = {{2013 IEEE International Conference on Computer Vision: 1-8 December 2013}},
 keywords = {Computer vision},
 address = {New York},
 publisher = {IEEE},
 isbn = {978-1-4799-2840-8}
}


@proceedings{27.06.201630.06.2016,
 year = {27.06.2016 - 30.06.2016},
 title = {{2016 IEEE Conference on Computer Vision and Pattern Recognition (CVPR)}},
 publisher = {IEEE},
 isbn = {978-1-4673-8851-1}
}


@book{Cremers2015,
 year = {2015},
 title = {{Computer Vision -- ACCV 2014}},
 address = {Cham},
 publisher = {{Springer International Publishing}},
 isbn = {978-3-319-16864-7},
 series = {{Lecture Notes in Computer Science}},
 editor = {Cremers, Daniel and Reid, Ian and Saito, Hideo and Yang, Ming-Hsuan},
 doi = {10.1007/978-3-319-16865-4}
}


@book{Cremers2015b,
 year = {2015},
 title = {{Computer Vision -- ACCV 2014}},
 address = {Cham},
 publisher = {{Springer International Publishing}},
 isbn = {978-3-319-16864-7},
 series = {{Lecture Notes in Computer Science}},
 editor = {Cremers, Daniel and Reid, Ian and Saito, Hideo and Yang, Ming-Hsuan},
 doi = {10.1007/978-3-319-16865-4}
}


@inproceedings{Devineau.2018,
 crossref = {IEEEInternationalConferenceonAutomaticFaceandGestureRecognition2018},
 author = {Devineau, Guillaume and Moutarde, Fabien and Xi, Wang and Yang, Jie},
 title = {{Deep Learning for Hand Gesture Recognition on Skeletal Data}},
 pages = {106--113},
 publisher = {IEEE},
 isbn = {978-1-5386-2335-0},
 booktitle = {{13th IEEE International Conference on Automatic Face and Gesture Recognition}},
 year = {2018},
 address = {Piscataway, NJ},
 doi = {10.1109/FG.2018.00025}
}


@article{DiWu2016,
 abstract = {This paper describes a novel method called Deep Dynamic Neural Networks (DDNN) for multimodal gesture recognition. A semi-supervised hierarchical dynamic framework based on a Hidden Markov Model (HMM) is proposed for simultaneous gesture segmentation and recognition where skeleton joint information, depth and RGB images, are the multimodal input observations. Unlike most traditional approaches that rely on the construction of complex handcrafted features, our approach learns high-level spatio-temporal representations using deep neural networks suited to the input modality: a Gaussian-Bernouilli Deep Belief Network (DBN) to handle skeletal dynamics, and a 3D Convolutional Neural Network (3DCNN) to manage and fuse batches of depth and RGB images. This is achieved through the modeling and learning of the emission probabilities of the HMM required to infer the gesture sequence. This purely data driven approach achieves a Jaccard index score of 0.81 in the ChaLearn LAP gesture spotting challenge. The performance is on par with a variety of state-of-the-art hand-tuned feature-based approaches and other learning-based methods, therefore opening the door to the use of deep learning techniques in order to further explore multimodal time series data.},
 author = {{Di Wu} and Pigou, Lionel and Kindermans, Pieter-Jan and Le, Nam Do-Hoang and Shao, Ling and Dambre, Joni and Odobez, Jean-Marc},
 year = {2016},
 title = {{Deep Dynamic Neural Networks for Multimodal Gesture Segmentation and Recognition}},
 keywords = {Algorithms;Gestures;Humans;Learning;Neural Networks (Computer);Normal Distribution;Pattern Recognition, Automated},
 pages = {1583--1597},
 volume = {38},
 number = {8},
 journal = {{IEEE transactions on pattern analysis and machine intelligence}},
 doi = {10.1109/TPAMI.2016.2537340}
}


@article{FeiFei,
 author = {Fei-Fei, Li},
 title = {{Knowledge transfer in learning to recognize visual objects classes}}
}


@book{Fitzgibbon2012,
 year = {2012},
 title = {{Computer vision - ECCV 2012: 12th European Conference on Computer Vision, Florence, Italy, October 7 - 13, 2012 ; proceedings, part VI}},
 keywords = {Artificial intelligence;Computer graphics;Computer science;Computer software;Computer vision;Optical pattern recognition},
 address = {Berlin},
 volume = {7577},
 publisher = {Springer},
 isbn = {978-3-642-33782-6},
 series = {{Lecture Notes in Computer Science}},
 editor = {Fitzgibbon, Andrew and Lazebnik, Svetlana and Perona, Pietro and Sato, Yoichi and Schmid, Cordelia},
 doi = {10.1007/978-3-642-33783-3}
}


@article{Ge2019,
 abstract = {In this paper, we present a novel method for real-time 3D hand pose estimation from single depth images using 3D Convolutional Neural Networks (CNNs). Image-based features extracted by 2D CNNs are not directly suitable for 3D hand pose estimation due to the lack of 3D spatial information. Our proposed 3D CNN-based method, taking a 3D volumetric representation of the hand depth image as input and extracting 3D features from the volumetric input, can capture the 3D spatial structure of the hand and accurately regress full 3D hand pose in a single pass. In order to make the 3D CNN robust to variations in hand sizes and global orientations, we perform 3D data augmentation on the training data. To further improve the estimation accuracy, we propose applying the 3D deep network architectures and leveraging the complete hand surface as intermediate supervision for learning 3D hand pose from depth images. Extensive experiments on three challenging datasets demonstrate that our proposed approach outperforms baselines and state-of-the-art methods. A cross-dataset experiment also shows that our method has good generalization ability. Furthermore, our method is fast as our implementation runs at over 91 frames per second on a standard computer with a single GPU.},
 author = {Ge, Liuhao and Liang, Hui and Yuan, Junsong and Thalmann, Daniel},
 year = {2019},
 title = {{Real-Time 3D Hand Pose Estimation with 3D Convolutional Neural Networks}},
 pages = {956--970},
 volume = {41},
 number = {4},
 journal = {{IEEE transactions on pattern analysis and machine intelligence}},
 doi = {10.1109/TPAMI.2018.2827052}
}


@proceedings{Hoey2011,
 year = {2011},
 title = {{Proceedings of the British Machine Vision Conference 2011: BMVC 2011, the 22nd British Machine Vision Conference, University of Dundee, 29 August-2 September 2011}},
 address = {Durham},
 publisher = {{BMVA Press}},
 isbn = {1-901725-43-X},
 editor = {Hoey, Jesse},
 institution = {{British Machine Vision Conference} and BMVC},
 doi = {10.5244/C.25}
}


@proceedings{IEEEConferenceonComputerVisionandPatternRecognition2016,
 year = {2016},
 title = {{29th IEEE Conference on Computer Vision and Pattern Recognition: CVPR 2016 : proceedings : 26 June-1 July 2016, Las Vegas, Nevada}},
 keywords = {Maschinelles Sehen;Mustererkennung},
 address = {Piscataway, NJ},
 publisher = {IEEE},
 isbn = {978-1-4673-8851-1},
 institution = {{IEEE Conference on Computer Vision and Pattern Recognition} and {Institute of Electrical and Electronics Engineers} and CVPR}
}


@proceedings{InstituteofElectricalandElectronicsEngineers2013,
 year = {2013},
 title = {{IEEE Conference on Computer Vision and Pattern Recognition (CVPR), 2013: 23 - 28 June 2013, Portland, Oregon, USA ; proceedings}},
 keywords = {Maschinelles Sehen;Mustererkennung},
 address = {Piscataway, NJ},
 publisher = {IEEE},
 isbn = {978-0-7695-4989-7},
 institution = {{Institute of Electrical and Electronics Engineers} and {IEEE Computer Society} and {IEEE Conference on Computer Vision and Pattern Recognition} and CVPR}
}


@article{Jones2002,
 abstract = {The existence of large image datasets such as the set of photos on the World Wide Web make it

possible to build powerful generic models for low-level image attributes like color using simple histogram learning

techniques. We describe the construction of color models for skin and non-skin classes from a dataset of nearly

1 billion labelled pixels. These classes exhibit a surprising degree of separability which we exploit by building a skin

pixel detector achieving a detection rate of 80{\%} with 8.5{\%} false positives. We compare the performance of histogram

and mixture models in skin detection and ﬁnd histogram models to be superior in accuracy and computational cost.

Using aggregate features computed from the skin pixel detector we build a surprisingly effective detector for naked

people. Our results suggest that color can be a more powerful cue for detecting people in unconstrained imagery

than was previously suspected. We believe this work is the most comprehensive and detailed exploration of skin

color models to date.},
 author = {Jones, Michael J. and Rehg, James M.},
 year = {2002},
 title = {{Statistical Color Models with Application to Skin Detection}},
 pages = {81--96},
 volume = {46},
 number = {1},
 issn = {0920-5691},
 journal = {{International Journal of Computer Vision}},
 doi = {10.1023/A:1013200319198}
}


@book{Kang,
 author = {Kang, Sing Bing and Szeliski, Richard},
 title = {{Extracting View-Dependent Depth Maps from a Collection of Images}},
 keywords = {Artificial Intelligence (incl. Robotics);Computer Imaging, Vision, Pattern Recognition and Graphics;Image Processing and Computer Vision;Pattern Recognition},
 institution = {{SpringerLink (Online service)}}
}


@incollection{Keskin2012,
 crossref = {Fitzgibbon2012},
 author = {Keskin, Cem and K{\i}ra{\c{c}}, Furkan and Kara, Yunus Emre and Akarun, Lale},
 title = {{Hand Pose Estimation and Hand Shape Classification Using Multi-layered Randomized Decision Forests}},
 pages = {852--863},
 volume = {7577},
 publisher = {Springer},
 isbn = {978-3-642-33782-6},
 series = {{Lecture Notes in Computer Science}},
 editor = {Fitzgibbon, Andrew and Lazebnik, Svetlana and Perona, Pietro and Sato, Yoichi and Schmid, Cordelia},
 booktitle = {{Computer vision - ECCV 2012}},
 year = {2012},
 address = {Berlin},
 doi = {10.1007/978-3-642-33783-3{\textunderscore }61}
}


@inproceedings{Li2013,
 crossref = {InstituteofElectricalandElectronicsEngineers2013},
 author = {Li, Cheng and Kitani, Kris M.},
 title = {{Pixel-Level Hand Detection in Ego-centric Videos}},
 pages = {3570--3577},
 publisher = {IEEE},
 isbn = {978-0-7695-4989-7},
 booktitle = {{IEEE Conference on Computer Vision and Pattern Recognition (CVPR), 2013}},
 year = {2013},
 address = {Piscataway, NJ},
 doi = {10.1109/CVPR.2013.458}
}


@inproceedings{Molchanov27.06.201630.06.2016,
 crossref = {27.06.201630.06.2016},
 abstract = {2016 IEEE Conference on Computer Vision and Pattern Recognition (CVPR);2016; ; ;10.1109/CVPR.2016.456},
 author = {Molchanov, Pavlo and Yang, Xiaodong and Gupta, Shalini and Kim, Kihwan and Tyree, Stephen and Kautz, Jan},
 title = {{Online Detection and Classification of Dynamic Hand Gestures with Recurrent 3D Convolutional Neural Networks}},
 keywords = {Nvidia},
 pages = {4207--4215},
 publisher = {IEEE},
 isbn = {978-1-4673-8851-1},
 booktitle = {{2016 IEEE Conference on Computer Vision and Pattern Recognition (CVPR)}},
 year = {27.06.2016 - 30.06.2016},
 doi = {10.1109/CVPR.2016.456}
}


@article{OhnBar2014,
 author = {Ohn-Bar, Eshed and Trivedi, Mohan Manubhai},
 year = {2014},
 title = {{Hand Gesture Recognition in Real Time for Automotive Interfaces: A Multimodal Vision-Based Approach and Evaluations}},
 pages = {2368--2377},
 volume = {15},
 number = {6},
 issn = {1524-9050},
 journal = {{IEEE Transactions on Intelligent Transportation Systems}},
 doi = {10.1109/TITS.2014.2337331}
}


@inproceedings{Oikonomidis2011,
 crossref = {Hoey2011},
 author = {Oikonomidis, Iason and Kyriazis, Nikolaos and Argyros, Antonis},
 title = {{Efficient model-based 3D tracking of hand articulations using Kinect}},
 pages = {101.1--101.11},
 publisher = {{BMVA Press}},
 isbn = {1-901725-43-X},
 editor = {Hoey, Jesse},
 booktitle = {{Proceedings of the British Machine Vision Conference 2011}},
 year = {2011},
 address = {Durham},
 doi = {10.5244/C.25.101}
}


@incollection{Pfister2015,
 crossref = {Cremers2015},
 author = {Pfister, Tomas and Simonyan, Karen and Charles, James and Zisserman, Andrew},
 title = {{Deep Convolutional Neural Networks for Efficient Pose Estimation in Gesture Videos}},
 pages = {538--552},
 volume = {9003},
 publisher = {{Springer International Publishing}},
 isbn = {978-3-319-16864-7},
 series = {{Lecture Notes in Computer Science}},
 editor = {Cremers, Daniel and Reid, Ian and Saito, Hideo and Yang, Ming-Hsuan},
 booktitle = {{Computer Vision -- ACCV 2014}},
 year = {2015},
 address = {Cham},
 doi = {10.1007/978-3-319-16865-4{\textunderscore }35}
}


@inproceedings{Poier2015,
 crossref = {Xie2015},
 author = {Poier, Georg and Roditakis, Konstantinos and Schulter, Samuel and Michel, Damien and Bischof, Horst and Argyros, Antonis A.},
 title = {{Hybrid One-Shot 3D Hand Pose Estimation by Exploiting Uncertainties}},
 pages = {182.1--182.14},
 publisher = {{BMVA Press}},
 isbn = {1-901725-53-7},
 editor = {Xie, Xianghua and Jones, Mark W. and Tam, Gary K. L.},
 booktitle = {{Proceedings of the British Machine Vision Conference 2015}},
 year = {2015},
 address = {Durham},
 doi = {10.5244/C.29.182}
}


@inproceedings{Sato1317March2001,
 crossref = {1317March2001},
 abstract = {Proceedings IEEE Virtual Reality 2001;2001; ; ;10.1109/VR.2001.913773},
 author = {Sato, Y. and Saito, M. and Koike, H.},
 title = {{Real-time input of 3D pose and gestures of a user's hand and its applications for HCI}},
 pages = {79--86},
 publisher = {{IEEE Comput. Soc}},
 isbn = {0-7695-0948-7},
 booktitle = {{Proceedings IEEE Virtual Reality 2001}},
 year = {13-17 March 2001},
 doi = {10.1109/VR.2001.913773}
}


@book{Scharstein,
 author = {Scharstein, Daniel and Szeliski, Richard},
 title = {{Stereo Matching with Nonlinear Diffusion}},
 keywords = {Artificial Intelligence (incl. Robotics);Computer Imaging, Vision, Pattern Recognition and Graphics;Image Processing and Computer Vision;Pattern Recognition},
 institution = {{SpringerLink (Online service)}}
}


@inproceedings{Sridhar2013,
 crossref = {2013},
 author = {Sridhar, Srinath and Oulasvirta, Antti and Theobalt, Christian},
 title = {{Interactive Markerless Articulated Hand Motion Tracking using RGB and Depth Data}},
 url = {http://handtracker.mpi-inf.mpg.de/projects/handtracker_iccv2013/},
 booktitle = {{Proceedings of the IEEE International Conference on Computer Vision (ICCV)}},
 year = {2013}
}


@phdthesis{Stark,
 author = {Stark, Michael},
 title = {{On knowledge transfer in object class recognition}},
 type = {{Darmstadt, Techn. Univ., Diss., 2010}}
}


@article{Supancic2018,
 author = {Supan{\v{c}}i{\v{c}}, James Steven and Rogez, Gr{\'e}gory and Yang, Yi and Shotton, Jamie and Ramanan, Deva},
 year = {2018},
 title = {{Depth-Based Hand Pose Estimation: Methods, Data, and Challenges}},
 pages = {1180--1198},
 volume = {126},
 number = {11},
 issn = {0920-5691},
 journal = {{International Journal of Computer Vision}},
 doi = {10.1007/s11263-018-1081-7}
}


@book{Szeliski1989,
 author = {Szeliski, Richard},
 year = {1989},
 title = {{Bayesian modeling of uncertainty in low-level vision}},
 keywords = {Bayes-Verfahren;Bildverarbeitung},
 isbn = {0792390393},
 series = {{The Kluwer international series in engineering and computer science}}
}


@book{Szeliski2006,
 author = {Szeliski, Richard},
 year = {2006},
 title = {{Image alignment and stitching: A Tutorial}},
 keywords = {Algorithmus;Bildverarbeitung},
 address = {Hanover Mass. u.a.},
 volume = {2,1},
 publisher = {{Now Publ}},
 isbn = {1-933019-04-2},
 series = {{Foundations and trends in computer graphics and vision}}
}


@book{Szeliski2010,
 author = {Szeliski, Richard},
 year = {2010},
 title = {{Computer Vision: Algorithms and Applications}},
 address = {Guildford, Surrey},
 edition = {1. Aufl.},
 publisher = {{Springer London}},
 isbn = {9781848829343},
 series = {{Texts in computer science}}
}


@book{Szeliski2011,
 author = {Szeliski, Richard},
 year = {2011},
 title = {{Computer vision: Algorithms and applications}},
 price = {EUR 80.20},
 keywords = {Bildverarbeitung;Lehrbuch;Maschinelles Sehen},
 isbn = {9781848829343},
 series = {{Texts in computer science}}
}


@book{Szeliski2011b,
 author = {Szeliski, Richard},
 year = {2011},
 title = {{Computer Vision: Algorithms and Applications}},
 address = {London},
 publisher = {{Springer London}},
 isbn = {9781848829350},
 series = {{Texts in computer science}}
}


@inproceedings{Tang2014,
 crossref = {2014},
 author = {Tang, Danhang and Yu, Tsz-Ho and Kim, Tae-Kyun},
 title = {{Real-Time Articulated Hand Pose Estimation Using Semi-supervised Transductive Regression Forests}},
 pages = {3224--3231},
 publisher = {IEEE},
 isbn = {978-1-4799-2840-8},
 booktitle = {{2013 IEEE International Conference on Computer Vision}},
 year = {2014},
 address = {New York},
 doi = {10.1109/ICCV.2013.400}
}


@article{Tompson2014,
 author = {Tompson, Jonathan and Stein, Murphy and Lecun, Yann and Perlin, Ken},
 year = {2014},
 title = {{Real-Time Continuous Pose Recovery of Human Hands Using Convolutional Networks}},
 pages = {1--10},
 volume = {33},
 number = {5},
 issn = {07300301},
 journal = {{ACM Transactions on Graphics}},
 doi = {10.1145/2629500}
}


@misc{UniFreiburg,
 author = {{Uni Freiburg}},
 title = {{Rendered Handpose Dataset}},
 url = {https://lmb.informatik.uni-freiburg.de/resources/datasets/RenderedHandposeDataset.en.html},
 publisher = {{Uni Freiburg}}
}


@article{Wang.2016,
 author = {Wang, Pichao and Li, Wanqing and Gao, Zhimin and Zhang, Jing and Tang, Chang and Ogunbona, Philip O.},
 year = {2016},
 title = {{Action Recognition From Depth Maps Using Deep Convolutional Neural Networks}},
 pages = {498--509},
 volume = {46},
 number = {4},
 issn = {2168-2291},
 journal = {{IEEE Transactions on Human-Machine Systems}},
 doi = {10.1109/THMS.2015.2504550}
}


@proceedings{Xie2015,
 year = {2015},
 title = {{Proceedings of the British Machine Vision Conference 2015: BMVC 2015, 7-10 September, Swansea, UK}},
 address = {Durham},
 publisher = {{BMVA Press}},
 isbn = {1-901725-53-7},
 editor = {Xie, Xianghua and Jones, Mark W. and Tam, Gary K. L.},
 institution = {{British Machine Vision Conference} and BMVC},
 doi = {10.5244/C.29}
}


@misc{Zhang23.10.2016,
 abstract = {3D hand pose tracking/estimation will be very important in the next generation of human-computer interaction. Most of the currently available algorithms rely on low-cost active depth sensors. However, these sensors can be easily interfered by other active sources and require relatively high power consumption. As a result, they are currently not suitable for outdoor environments and mobile devices. This paper aims at tracking/estimating hand poses using passive stereo which avoids these limitations. A benchmark with 18,000 stereo image pairs and 18,000 depth images captured from different scenarios and the ground-truth 3D positions of palm and finger joints (obtained from the manual label) is thus proposed. This paper demonstrates that the performance of the state-of-the art tracking/estimation algorithms can be maintained with most stereo matching algorithms on the proposed benchmark, as long as the hand segmentation is correct. As a result, a novel stereo-based hand segmentation algorithm specially designed for hand tracking/estimation is proposed. The quantitative evaluation demonstrates that the proposed algorithm is suitable for the state-of-the-art hand pose tracking/estimation algorithms and the tracking quality is comparable to the use of active depth sensors under different challenging scenarios.},
 author = {Zhang, Jiawei and Jiao, Jianbo and Chen, Mingliang and Qu, Liangqiong and Xu, Xiaobin and Yang, Qingxiong},
 date = {23.10.2016},
 title = {{3D Hand Pose Tracking and Estimation Using Stereo Matching}},
 url = {http://arxiv.org/pdf/1610.07214v1},
 keywords = {Computer Science - Computer Vision and Pattern Recognition}
}


@misc{Zimmermann03.05.2017,
 abstract = {Low-cost consumer depth cameras and deep learning have enabled reasonable 3D hand pose estimation from single depth images. In this paper, we present an approach that estimates 3D hand pose from regular RGB images. This task has far more ambiguities due to the missing depth information. To this end, we propose a deep network that learns a network-implicit 3D articulation prior. Together with detected keypoints in the images, this network yields good estimates of the 3D pose. We introduce a large scale 3D hand pose dataset based on synthetic hand models for training the involved networks. Experiments on a variety of test sets, including one on sign language recognition, demonstrate the feasibility of 3D hand pose estimation on single color images.},
 author = {Zimmermann, Christian and Brox, Thomas},
 date = {03.05.2017},
 title = {{Learning to Estimate 3D Hand Pose from Single RGB Images}},
 url = {http://arxiv.org/pdf/1705.01389v3},
 keywords = {Algorithms;Algorithmus;Artificial Intelligence (incl. Robotics);Bayes-Verfahren;Bildverarbeitung;Computer Imaging, Vision, Pattern Recognition and Graphics;Computer Science - Computer Vision and Pattern Recognition;Dataset;Gestures;Humans;Image Processing and Computer Vision;Learning;Lehrbuch;Maschinelles Sehen;Neural Networks (Computer);Normal Distribution;Pattern Recognition;Pattern Recognition, Automated}
}


