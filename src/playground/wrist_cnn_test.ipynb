{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true,
    "pycharm": {
     "is_executing": false
    }
   },
   "outputs": [],
   "source": [
    "import tools\n",
    "import os\n",
    "import tensorflow as tf\n",
    "import models\n",
    "\n",
    "__logger = tools.get_logger(__name__, do_file_logging=False)\n",
    "__checkpoint_file_prefix = \"wrist_est\"\n",
    "data_dir = \"E:\\\\MasterDaten\\\\Results\\\\wrist_cnn\"\n",
    "checkpoint_dir = os.path.join(data_dir, \"checkpoints\")\n",
    "saved_model_dir = os.path.join(data_dir, \"saved_models\")\n",
    "log_dir = os.path.join(data_dir, \"logs\")\n",
    "tensorboard_dir = os.path.join(log_dir, \"tensorboard\")\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "outputs": [],
   "source": [
    "def try_load_checkpoint(model, checkpoint_dir):\n",
    "    if os.path.exists(checkpoint_dir):\n",
    "        cp_files = [os.path.abspath(os.path.join(checkpoint_dir, filename)) for filename in os.listdir(checkpoint_dir)]\n",
    "        cp_files = [path for path in cp_files if os.path.isfile(path) and __checkpoint_file_prefix in os.path.basename(path)]\n",
    "\n",
    "        if len(cp_files) > 0:\n",
    "            files_sorted = sorted(cp_files, key=os.path.getctime, reverse=True)\n",
    "            for latest_file in files_sorted:\n",
    "                try:\n",
    "                    __logger.info(\"Trying to load weights from {}\".format(latest_file))\n",
    "                    model.load_weights(latest_file)\n",
    "                    __logger.info(\"Loading successful!\")\n",
    "                    return model\n",
    "                except Exception as e:\n",
    "                    __logger.exception(e)\n",
    "                    __logger.error(\"Loading of {} failed!\".format(latest_file))\n",
    "\n",
    "def save(model, save_name=None):\n",
    "    if not os.path.exists(saved_model_dir):\n",
    "        os.makedirs(saved_model_dir)\n",
    "\n",
    "    if save_name:\n",
    "        tf.saved_model.save(model, os.path.join(saved_model_dir, save_name))\n",
    "    else:\n",
    "        highest_index = max([d for d in os.listdir(saved_model_dir) if not os.path.isfile(os.path.join(saved_model_dir, d) and d.isnumeric())])\n",
    "        tf.saved_model.save(model, os.path.join(saved_model_dir, str(highest_index + 1)))\n",
    "    return model\n",
    "\n",
    "def try_load_saved(model, save_name=None):\n",
    "    if not os.path.exists(saved_model_dir):\n",
    "        return False\n",
    "\n",
    "    if save_name:\n",
    "        model = tf.saved_model.load(os.path.join(saved_model_dir, save_name))\n",
    "    else:\n",
    "        highest_index = max([d for d in os.listdir(saved_model_dir) if not os.path.isfile(os.path.join(saved_model_dir, d) and d.isnumeric())])\n",
    "        model = tf.saved_model.save(os.path.join(saved_model_dir, str(highest_index + 1)))\n",
    "    return model"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n",
     "is_executing": false
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "outputs": [
    {
     "name": "stderr",
     "text": [
      "\nKeyboardInterrupt\n\n"
     ],
     "output_type": "stream"
    }
   ],
   "source": [
    "model = models.WristCNN()\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n",
     "is_executing": false
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "def prepare_ds(ds):\n",
    "    ds = ds.map(lambda img, skel: (\n",
    "            tf.clip_by_value(tf.cast(tf.image.resize(img, tf.constant([227, 227], dtype=tf.dtypes.int32)),\n",
    "                                     dtype=tf.float32) / tf.constant(2500.0, dtype=tf.float32),\n",
    "                             clip_value_min=0.0,\n",
    "                             clip_value_max=1.0),\n",
    "            skel[:3]), num_parallel_calls=tf.data.experimental.AUTOTUNE)  # ignore stuff more than 2.5m away.\n",
    "    ds = ds.map(\n",
    "            lambda img, skel: (\n",
    "                    tools.colorize(img, 0.0, 1.0, \"viridis\"),\n",
    "                    skel\n",
    "            )\n",
    "    )\n",
    "    return ds\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n",
     "is_executing": false
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "dataset_train = tools.NYU.get_dataset(\"G:\\\\master_thesis_data\\\\Datasets\\\\nyu\\\\nyu_hand_dataset_v2\\\\dataset\", \"train\")\n",
    "dataset_validation = tools.NYU.get_dataset(\"G:\\\\master_thesis_data\\\\Datasets\\\\nyu\\\\nyu_hand_dataset_v2\\\\dataset\", \"validation\")\n",
    "\n",
    "dataset_train = prepare_ds(dataset_train)\n",
    "dataset_validation = prepare_ds(dataset_validation)\n",
    "\n",
    "dataset_train = dataset_train.shuffle(10 * 10)\n",
    "dataset_train = dataset_train.batch(batch_size=25).prefetch(tf.data.experimental.AUTOTUNE)\n",
    "\n",
    "dataset_validation = dataset_validation.batch(batch_size=25).prefetch(tf.data.experimental.AUTOTUNE)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n",
     "is_executing": false
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "learning_rate = 0.0005\n",
    "tools.clean_tensorboard_logs(tensorboard_dir)\n",
    "\n",
    "optimizer = tf.keras.optimizers.Adam(learning_rate=learning_rate, clipvalue=10)\n",
    "model.compile(optimizer=optimizer, loss=tf.keras.losses.mean_squared_error, metrics=[\"mae\", \"acc\"])\n",
    "\n",
    "if not os.path.exists(checkpoint_dir): os.makedirs(checkpoint_dir)\n",
    "if not os.path.exists(tensorboard_dir): os.makedirs(tensorboard_dir)\n",
    "\n",
    "checkpointer = tf.keras.callbacks.ModelCheckpoint(\n",
    "        filepath=os.path.join(checkpoint_dir, __checkpoint_file_prefix + \"weights.{epoch:02d}.hdf5\"),\n",
    "        save_best_only=False)\n",
    "\n",
    "tensorboard = tf.keras.callbacks.TensorBoard(log_dir=tensorboard_dir, histogram_freq=0,\n",
    "                                             write_graph=True, write_images=True, update_freq='batch', profile_batch=0)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n",
     "is_executing": false
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "model.fit(\n",
    "        dataset_train,\n",
    "        validation_data=dataset_validation,\n",
    "        epochs=300,\n",
    "        verbose=2,\n",
    "        callbacks=[checkpointer, tensorboard])"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n",
     "is_executing": false
    }
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  },
  "pycharm": {
   "stem_cell": {
    "cell_type": "raw",
    "source": [],
    "metadata": {
     "collapsed": false
    }
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}